<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Tutorial: PINA and PyTorch Lightning, training tips and visualizations &mdash; PINA 0.1.0.post2405 documentation</title><link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/language_data.js"></script>
        <script src="../../../_static/clipboard.min.js"></script>
        <script src="../../../_static/copybutton.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Tutorial: Building custom geometries with PINA Location class" href="../tutorial6/tutorial.html" />
    <link rel="prev" title="Tutorial: The Equation Class" href="../tutorial12/tutorial.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html">
            
              <img src="../../../_static/pina_logo.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                0.1.0.post2405
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Package Documentation:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../_code.html">API</a></li>
</ul>
<p class="caption"><span class="caption-text">Getting Started:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../_installation.html">Installation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../_tutorial.html">Tutorials</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../../_tutorial.html#getting-started-with-pina">Getting started with PINA</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../tutorial1/tutorial.html">Introduction to PINA for Physics Informed Neural Networks training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial12/tutorial.html">Introduction to PINA Equation class</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">PINA and PyTorch Lightning, training tips and visualizations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#trainer-accelerator">Trainer Accelerator</a></li>
<li class="toctree-l4"><a class="reference internal" href="#trainer-logging">Trainer Logging</a></li>
<li class="toctree-l4"><a class="reference internal" href="#trainer-callbacks">Trainer Callbacks</a></li>
<li class="toctree-l4"><a class="reference internal" href="#trainer-tips-to-boost-accuracy-save-memory-and-speed-up-training">Trainer Tips to Boost Accuracy, Save Memory and Speed Up Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="#whats-next">What’s next?</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial6/tutorial.html">Building custom geometries with PINA Location class</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../_tutorial.html#physics-informed-neural-networks">Physics Informed Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_tutorial.html#neural-operator-learning">Neural Operator Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_tutorial.html#supervised-learning">Supervised Learning</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Community:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../_team.html">Team &amp; Fundings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../_LICENSE.html">License</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../_cite.html">Cite PINA</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">PINA</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../_tutorial.html">PINA Tutorials</a></li>
      <li class="breadcrumb-item active">Tutorial: PINA and PyTorch Lightning, training tips and visualizations</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/_rst/tutorials/tutorial11/tutorial.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="tutorial-pina-and-pytorch-lightning-training-tips-and-visualizations">
<h1>Tutorial: PINA and PyTorch Lightning, training tips and visualizations<a class="headerlink" href="#tutorial-pina-and-pytorch-lightning-training-tips-and-visualizations" title="Permalink to this headline">¶</a></h1>
<p>In this tutorial, we will delve deeper into the functionality of the
<code class="docutils literal notranslate"><span class="pre">Trainer</span></code> class, which serves as the cornerstone for training <strong>PINA</strong>
<a class="reference external" href="https://mathlab.github.io/PINA/_rst/_code.html#solvers">Solvers</a>.
The <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> class offers a plethora of features aimed at improving
model accuracy, reducing training time and memory usage, facilitating
logging visualization, and more thanks to the amazing job done by the PyTorch Lightning team!
Our leading example will revolve around solving the <code class="docutils literal notranslate"><span class="pre">SimpleODE</span></code>
problem, as outlined in the <a class="reference external" href="https://github.com/mathLab/PINA/blob/master/tutorials/tutorial1/tutorial.ipynb">Introduction to PINA for Physics Informed
Neural Networks
training</a>.
If you haven’t already explored it, we highly recommend doing so before
diving into this tutorial.
Let’s start by importing useful modules, define the <code class="docutils literal notranslate"><span class="pre">SimpleODE</span></code>
problem and the <code class="docutils literal notranslate"><span class="pre">PINN</span></code> solver.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import torch

from pina import Condition, Trainer
from pina.solvers import PINN
from pina.model import FeedForward
from pina.problem import SpatialProblem
from pina.operators import grad
from pina.geometry import CartesianDomain
from pina.equation import Equation, FixedValue

class SimpleODE(SpatialProblem):

    output_variables = [&#39;u&#39;]
    spatial_domain = CartesianDomain({&#39;x&#39;: [0, 1]})

    # defining the ode equation
    def ode_equation(input_, output_):
        u_x = grad(output_, input_, components=[&#39;u&#39;], d=[&#39;x&#39;])
        u = output_.extract([&#39;u&#39;])
        return u_x - u

    # conditions to hold
    conditions = {
        &#39;x0&#39;: Condition(location=CartesianDomain({&#39;x&#39;: 0.}), equation=FixedValue(1)),             # We fix initial condition to value 1
        &#39;D&#39;: Condition(location=CartesianDomain({&#39;x&#39;: [0, 1]}), equation=Equation(ode_equation)), # We wrap the python equation using Equation
    }

    # defining the true solution
    def truth_solution(self, pts):
        return torch.exp(pts.extract([&#39;x&#39;]))


# sampling for training
problem = SimpleODE()
problem.discretise_domain(1, &#39;random&#39;, locations=[&#39;x0&#39;])
problem.discretise_domain(20, &#39;lh&#39;, locations=[&#39;D&#39;])

# build the model
model = FeedForward(
    layers=[10, 10],
    func=torch.nn.Tanh,
    output_dimensions=len(problem.output_variables),
    input_dimensions=len(problem.input_variables)
)

# create the PINN object
pinn = PINN(problem, model)
</pre></div>
</div>
<p>Till now we just followed the extact step of the previous tutorials. The
<code class="docutils literal notranslate"><span class="pre">Trainer</span></code> object can be initialized by simiply passing the <code class="docutils literal notranslate"><span class="pre">PINN</span></code>
solver</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>trainer = Trainer(solver=pinn)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">GPU</span> <span class="n">available</span><span class="p">:</span> <span class="kc">True</span> <span class="p">(</span><span class="n">mps</span><span class="p">),</span> <span class="n">used</span><span class="p">:</span> <span class="kc">True</span>
<span class="n">TPU</span> <span class="n">available</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="n">using</span><span class="p">:</span> <span class="mi">0</span> <span class="n">TPU</span> <span class="n">cores</span>
<span class="n">IPU</span> <span class="n">available</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="n">using</span><span class="p">:</span> <span class="mi">0</span> <span class="n">IPUs</span>
<span class="n">HPU</span> <span class="n">available</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="n">using</span><span class="p">:</span> <span class="mi">0</span> <span class="n">HPUs</span>
</pre></div>
</div>
<div class="section" id="trainer-accelerator">
<h2>Trainer Accelerator<a class="headerlink" href="#trainer-accelerator" title="Permalink to this headline">¶</a></h2>
<p>When creating the trainer, <strong>by defualt</strong> the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> will choose
the most performing <code class="docutils literal notranslate"><span class="pre">accelerator</span></code> for training which is available in
your system, ranked as follow:</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://cloud.google.com/tpu/docs/intro-to-tpu">TPU</a></p></li>
<li><p><a class="reference external" href="https://www.graphcore.ai/products/ipu">IPU</a></p></li>
<li><p><a class="reference external" href="https://habana.ai/">HPU</a></p></li>
<li><p><a class="reference external" href="https://www.intel.com/content/www/us/en/products/docs/processors/what-is-a-gpu.html#:~:text=What%20does%20GPU%20stand%20for,video%20editing%2C%20and%20gaming%20applications">GPU</a> or <a class="reference external" href="https://developer.apple.com/metal/pytorch/">MPS</a></p></li>
<li><p>CPU</p></li>
</ol>
<p>For setting manually the <code class="docutils literal notranslate"><span class="pre">accelerator</span></code> run:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">accelerator</span> <span class="pre">=</span> <span class="pre">{'gpu',</span> <span class="pre">'cpu',</span> <span class="pre">'hpu',</span> <span class="pre">'mps',</span> <span class="pre">'cpu',</span> <span class="pre">'ipu'}</span></code> sets the
accelerator to a specific one</p></li>
</ul>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>trainer = Trainer(solver=pinn,
                  accelerator=&#39;cpu&#39;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">GPU</span> <span class="n">available</span><span class="p">:</span> <span class="kc">True</span> <span class="p">(</span><span class="n">mps</span><span class="p">),</span> <span class="n">used</span><span class="p">:</span> <span class="kc">False</span>
<span class="n">TPU</span> <span class="n">available</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="n">using</span><span class="p">:</span> <span class="mi">0</span> <span class="n">TPU</span> <span class="n">cores</span>
<span class="n">IPU</span> <span class="n">available</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="n">using</span><span class="p">:</span> <span class="mi">0</span> <span class="n">IPUs</span>
<span class="n">HPU</span> <span class="n">available</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="n">using</span><span class="p">:</span> <span class="mi">0</span> <span class="n">HPUs</span>
</pre></div>
</div>
<p>as you can see, even if in the used system <code class="docutils literal notranslate"><span class="pre">GPU</span></code> is available, it is
not used since we set <code class="docutils literal notranslate"><span class="pre">accelerator='cpu'</span></code>.</p>
</div>
<div class="section" id="trainer-logging">
<h2>Trainer Logging<a class="headerlink" href="#trainer-logging" title="Permalink to this headline">¶</a></h2>
<p>In <strong>PINA</strong> you can log metrics in different ways. The simplest approach
is to use the <code class="docutils literal notranslate"><span class="pre">MetricTraker</span></code> class from <code class="docutils literal notranslate"><span class="pre">pina.callbacks</span></code> as seen in
the <a class="reference external" href="https://github.com/mathLab/PINA/blob/master/tutorials/tutorial1/tutorial.ipynb">Introduction to PINA for Physics Informed Neural Networks
training</a>
tutorial.</p>
<p>However, expecially when we need to train multiple times to get an
average of the loss across multiple runs, <code class="docutils literal notranslate"><span class="pre">pytorch_lightning.loggers</span></code>
might be useful. Here we will use <code class="docutils literal notranslate"><span class="pre">TensorBoardLogger</span></code> (more on
<a class="reference external" href="https://lightning.ai/docs/pytorch/stable/extensions/logging.html">logging</a>
here), but you can choose the one you prefer (or make your own one).</p>
<p>We will now import <code class="docutils literal notranslate"><span class="pre">TensorBoardLogger</span></code>, do three runs of training and
then visualize the results. Notice we set <code class="docutils literal notranslate"><span class="pre">enable_model_summary=False</span></code>
to avoid model summary specifications (e.g. number of parameters), set
it to true if needed.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from pytorch_lightning.loggers import TensorBoardLogger

# three run of training, by default it trains for 1000 epochs
# we reinitialize the model each time otherwise the same parameters will be optimized
for _ in range(3):
    model = FeedForward(
        layers=[10, 10],
        func=torch.nn.Tanh,
        output_dimensions=len(problem.output_variables),
        input_dimensions=len(problem.input_variables)
    )
    pinn = PINN(problem, model)
    trainer = Trainer(solver=pinn,
                      accelerator=&#39;cpu&#39;,
                      logger=TensorBoardLogger(save_dir=&#39;simpleode&#39;),
                      enable_model_summary=False)
    trainer.train()
</pre></div>
</div>
<pre class="literal-block">GPU available: True (mps), used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs

<cite>Trainer.fit</cite> stopped: <cite>max_epochs=1000</cite> reached.
Epoch 999: 100%|██████████| 1/1 [00:00&lt;00:00, 133.46it/s, v_num=6, x0_loss=1.48e-5, D_loss=0.000655, mean_loss=0.000335]</pre>
<pre class="literal-block">GPU available: True (mps), used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs

<cite>Trainer.fit</cite> stopped: <cite>max_epochs=1000</cite> reached.
Epoch 999: 100%|██████████| 1/1 [00:00&lt;00:00, 154.49it/s, v_num=7, x0_loss=6.21e-6, D_loss=0.000221, mean_loss=0.000114]</pre>
<pre class="literal-block">GPU available: True (mps), used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs

<cite>Trainer.fit</cite> stopped: <cite>max_epochs=1000</cite> reached.
Epoch 999: 100%|██████████| 1/1 [00:00&lt;00:00, 62.60it/s, v_num=8, x0_loss=1.44e-5, D_loss=0.000572, mean_loss=0.000293]</pre>
<p>We can now visualize the logs by simply running
<code class="docutils literal notranslate"><span class="pre">tensorboard</span> <span class="pre">--logdir=simpleode/</span></code> on terminal, you should obtain a
webpage as the one shown below:</p>
<img alt="../../../_images/logging.png" src="../../../_images/logging.png" />
<p>as you can see, by default, <strong>PINA</strong> logs the losses which are shown in
the progress bar, as well as the number of epochs. You can always insert
more loggings by either defining a <strong>callback</strong> (<a class="reference external" href="https://lightning.ai/docs/pytorch/stable/extensions/callbacks.html">more on
callbacks</a>),
or inheriting the solver and modify the programs with different
<strong>hooks</strong> (<a class="reference external" href="https://lightning.ai/docs/pytorch/stable/common/lightning_module.html#hooks">more on
hooks</a>).</p>
</div>
<div class="section" id="trainer-callbacks">
<h2>Trainer Callbacks<a class="headerlink" href="#trainer-callbacks" title="Permalink to this headline">¶</a></h2>
<p>Whenever we need to access certain steps of the training for logging, do
static modifications (i.e. not changing the <code class="docutils literal notranslate"><span class="pre">Solver</span></code>) or updating
<code class="docutils literal notranslate"><span class="pre">Problem</span></code> hyperparameters (static variables), we can use
<code class="docutils literal notranslate"><span class="pre">Callabacks</span></code>. Notice that <code class="docutils literal notranslate"><span class="pre">Callbacks</span></code> allow you to add arbitrary
self-contained programs to your training. At specific points during the
flow of execution (hooks), the Callback interface allows you to design
programs that encapsulate a full set of functionality. It de-couples
functionality that does not need to be in <strong>PINA</strong> <code class="docutils literal notranslate"><span class="pre">Solver</span></code>s.
Lightning has a callback system to execute them when needed. Callbacks
should capture NON-ESSENTIAL logic that is NOT required for your
lightning module to run.</p>
<p>The following are best practices when using/designing callbacks.</p>
<ul class="simple">
<li><p>Callbacks should be isolated in their functionality.</p></li>
<li><p>Your callback should not rely on the behavior of other callbacks in
order to work properly.</p></li>
<li><p>Do not manually call methods from the callback.</p></li>
<li><p>Directly calling methods (eg. on_validation_end) is strongly
discouraged.</p></li>
<li><p>Whenever possible, your callbacks should not depend on the order in
which they are executed.</p></li>
</ul>
<p>We will try now to implement a naive version of <code class="docutils literal notranslate"><span class="pre">MetricTraker</span></code> to show
how callbacks work. Notice that this is a very easy application of
callbacks, fortunately in <strong>PINA</strong> we already provide more advanced
callbacks in <code class="docutils literal notranslate"><span class="pre">pina.callbacks</span></code>.</p>
<!-- Suppose we want to log the accuracy on some validation poit --><div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from pytorch_lightning.callbacks import Callback
import torch

# define a simple callback
class NaiveMetricTracker(Callback):
    def __init__(self):
        self.saved_metrics = []

    def on_train_epoch_end(self, trainer, __): # function called at the end of each epoch
        self.saved_metrics.append(
            {key: value for key, value in trainer.logged_metrics.items()}
        )
</pre></div>
</div>
<p>Let’s see the results when applyed to the <code class="docutils literal notranslate"><span class="pre">SimpleODE</span></code> problem. You can
define callbacks when initializing the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> by the <code class="docutils literal notranslate"><span class="pre">callbacks</span></code>
argument, which expects a list of callbacks.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model = FeedForward(
        layers=[10, 10],
        func=torch.nn.Tanh,
        output_dimensions=len(problem.output_variables),
        input_dimensions=len(problem.input_variables)
    )
pinn = PINN(problem, model)
trainer = Trainer(solver=pinn,
                  accelerator=&#39;cpu&#39;,
                  enable_model_summary=False,
                  callbacks=[NaiveMetricTracker()])  # adding a callbacks
trainer.train()
</pre></div>
</div>
<pre class="literal-block">GPU available: True (mps), used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs

<cite>Trainer.fit</cite> stopped: <cite>max_epochs=1000</cite> reached.
Epoch 999: 100%|██████████| 1/1 [00:00&lt;00:00, 149.27it/s, v_num=1, x0_loss=7.27e-5, D_loss=0.0016, mean_loss=0.000838]</pre>
<p>We can easily access the data by calling
<code class="docutils literal notranslate"><span class="pre">trainer.callbacks[0].saved_metrics</span></code> (notice the zero representing the
first callback in the list given at initialization).</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>trainer.callbacks[0].saved_metrics[:3] # only the first three epochs
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[{</span><span class="s1">&#39;x0_loss&#39;</span><span class="p">:</span> <span class="n">tensor</span><span class="p">(</span><span class="mf">0.9141</span><span class="p">),</span>
  <span class="s1">&#39;D_loss&#39;</span><span class="p">:</span> <span class="n">tensor</span><span class="p">(</span><span class="mf">0.0304</span><span class="p">),</span>
  <span class="s1">&#39;mean_loss&#39;</span><span class="p">:</span> <span class="n">tensor</span><span class="p">(</span><span class="mf">0.4722</span><span class="p">)},</span>
 <span class="p">{</span><span class="s1">&#39;x0_loss&#39;</span><span class="p">:</span> <span class="n">tensor</span><span class="p">(</span><span class="mf">0.8906</span><span class="p">),</span>
  <span class="s1">&#39;D_loss&#39;</span><span class="p">:</span> <span class="n">tensor</span><span class="p">(</span><span class="mf">0.0287</span><span class="p">),</span>
  <span class="s1">&#39;mean_loss&#39;</span><span class="p">:</span> <span class="n">tensor</span><span class="p">(</span><span class="mf">0.4596</span><span class="p">)},</span>
 <span class="p">{</span><span class="s1">&#39;x0_loss&#39;</span><span class="p">:</span> <span class="n">tensor</span><span class="p">(</span><span class="mf">0.8674</span><span class="p">),</span>
  <span class="s1">&#39;D_loss&#39;</span><span class="p">:</span> <span class="n">tensor</span><span class="p">(</span><span class="mf">0.0274</span><span class="p">),</span>
  <span class="s1">&#39;mean_loss&#39;</span><span class="p">:</span> <span class="n">tensor</span><span class="p">(</span><span class="mf">0.4474</span><span class="p">)}]</span>
</pre></div>
</div>
<p>PyTorch Lightning also has some built in <code class="docutils literal notranslate"><span class="pre">Callbacks</span></code> which can be used
in <strong>PINA</strong>, <a class="reference external" href="https://lightning.ai/docs/pytorch/stable/extensions/callbacks.html#built-in-callbacks">here an extensive
list</a>.</p>
<p>We can for example try the <code class="docutils literal notranslate"><span class="pre">EarlyStopping</span></code> routine, which
automatically stops the training when a specific metric converged (here
the <code class="docutils literal notranslate"><span class="pre">mean_loss</span></code>). In order to let the training keep going forever set
<code class="docutils literal notranslate"><span class="pre">max_epochs=-1</span></code>.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># ~2 mins
from pytorch_lightning.callbacks import EarlyStopping

model = FeedForward(
        layers=[10, 10],
        func=torch.nn.Tanh,
        output_dimensions=len(problem.output_variables),
        input_dimensions=len(problem.input_variables)
    )
pinn = PINN(problem, model)
trainer = Trainer(solver=pinn,
                  accelerator=&#39;cpu&#39;,
                  max_epochs = -1,
                  enable_model_summary=False,
                  callbacks=[EarlyStopping(&#39;mean_loss&#39;)])  # adding a callbacks
trainer.train()
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>GPU available: True (mps), used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs

Epoch 6157: 100%|██████████| 1/1 [00:00&lt;00:00, 139.84it/s, v_num=9, x0_loss=4.21e-9, D_loss=9.93e-6, mean_loss=4.97e-6]
</pre></div>
</div>
<p>As we can see the model automatically stop when the logging metric
stopped improving!</p>
</div>
<div class="section" id="trainer-tips-to-boost-accuracy-save-memory-and-speed-up-training">
<h2>Trainer Tips to Boost Accuracy, Save Memory and Speed Up Training<a class="headerlink" href="#trainer-tips-to-boost-accuracy-save-memory-and-speed-up-training" title="Permalink to this headline">¶</a></h2>
<p>Untill now we have seen how to choose the right <code class="docutils literal notranslate"><span class="pre">accelerator</span></code>, how to
log and visualize the results, and how to interface with the program in
order to add specific parts of code at specific points by <code class="docutils literal notranslate"><span class="pre">callbacks</span></code>.
Now, we well focus on how boost your training by saving memory and
speeding it up, while mantaining the same or even better degree of
accuracy!</p>
<p>There are several built in methods developed in PyTorch Lightning which
can be applied straight forward in <strong>PINA</strong>, here we report some:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/">Stochastic Weight
Averaging</a>
to boost accuracy</p></li>
<li><p><a class="reference external" href="https://deepgram.com/ai-glossary/gradient-clipping">Gradient
Clippling</a> to
reduce computational time (and improve accuracy)</p></li>
<li><p><a class="reference external" href="https://lightning.ai/docs/pytorch/stable/common/optimization.html#id3">Gradient
Accumulation</a>
to save memory consumption</p></li>
<li><p><a class="reference external" href="https://lightning.ai/docs/pytorch/stable/common/optimization.html#id3">Mixed Precision
Training</a>
to save memory consumption</p></li>
</ul>
<p>We will just demonstrate how to use the first two, and see the results
compared to a standard training. We use the
<a class="reference external" href="https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.Timer.html#lightning.pytorch.callbacks.Timer">Timer</a>
callback from <code class="docutils literal notranslate"><span class="pre">pytorch_lightning.callbacks</span></code> to take the times. Let’s
start by training a simple model without any optimization (train for
2000 epochs).</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from pytorch_lightning.callbacks import Timer
from pytorch_lightning import seed_everything

# setting the seed for reproducibility
seed_everything(42, workers=True)

model = FeedForward(
        layers=[10, 10],
        func=torch.nn.Tanh,
        output_dimensions=len(problem.output_variables),
        input_dimensions=len(problem.input_variables)
    )

pinn = PINN(problem, model)
trainer = Trainer(solver=pinn,
                  accelerator=&#39;cpu&#39;,
                  deterministic=True,  # setting deterministic=True ensure reproducibility when a seed is imposed
                  max_epochs = 2000,
                  enable_model_summary=False,
                  callbacks=[Timer()])  # adding a callbacks
trainer.train()
print(f&#39;Total training time {trainer.callbacks[0].time_elapsed(&quot;train&quot;):.5f} s&#39;)
</pre></div>
</div>
<pre class="literal-block">Seed set to 42
GPU available: True (mps), used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs


<cite>Trainer.fit</cite> stopped: <cite>max_epochs=2000</cite> reached.
Epoch 1999: 100%|██████████| 1/1 [00:00&lt;00:00, 163.58it/s, v_num=31, x0_loss=1.12e-6, D_loss=0.000127, mean_loss=6.4e-5]
Total training time 17.36381 s</pre>
<p>Now we do the same but with StochasticWeightAveraging</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from pytorch_lightning.callbacks import StochasticWeightAveraging

# setting the seed for reproducibility
seed_everything(42, workers=True)

model = FeedForward(
        layers=[10, 10],
        func=torch.nn.Tanh,
        output_dimensions=len(problem.output_variables),
        input_dimensions=len(problem.input_variables)
    )
pinn = PINN(problem, model)
trainer = Trainer(solver=pinn,
                  accelerator=&#39;cpu&#39;,
                  deterministic=True,
                  max_epochs = 2000,
                  enable_model_summary=False,
                  callbacks=[Timer(),
                             StochasticWeightAveraging(swa_lrs=0.005)])  # adding StochasticWeightAveraging callbacks
trainer.train()
print(f&#39;Total training time {trainer.callbacks[0].time_elapsed(&quot;train&quot;):.5f} s&#39;)
</pre></div>
</div>
<pre class="literal-block">Seed set to 42
GPU available: True (mps), used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs


Epoch 1598: 100%|██████████| 1/1 [00:00&lt;00:00, 210.04it/s, v_num=47, x0_loss=4.17e-6, D_loss=0.000204, mean_loss=0.000104]
Swapping scheduler <cite>ConstantLR</cite> for <cite>SWALR</cite>
<cite>Trainer.fit</cite> stopped: <cite>max_epochs=2000</cite> reached.
Epoch 1999: 100%|██████████| 1/1 [00:00&lt;00:00, 120.85it/s, v_num=47, x0_loss=1.56e-7, D_loss=7.49e-5, mean_loss=3.75e-5]
Total training time 17.10627 s</pre>
<p>As you can see, the training time does not change at all! Notice that
around epoch <code class="docutils literal notranslate"><span class="pre">1600</span></code> the scheduler is switched from the defalut one
<code class="docutils literal notranslate"><span class="pre">ConstantLR</span></code> to the Stochastic Weight Average Learning Rate
(<code class="docutils literal notranslate"><span class="pre">SWALR</span></code>). This is because by default <code class="docutils literal notranslate"><span class="pre">StochasticWeightAveraging</span></code>
will be activated after <code class="docutils literal notranslate"><span class="pre">int(swa_epoch_start</span> <span class="pre">*</span> <span class="pre">max_epochs)</span></code> with
<code class="docutils literal notranslate"><span class="pre">swa_epoch_start=0.7</span></code> by default. Finally, the final <code class="docutils literal notranslate"><span class="pre">mean_loss</span></code> is
lower when <code class="docutils literal notranslate"><span class="pre">StochasticWeightAveraging</span></code> is used.</p>
<p>We will now now do the same but clippling the gradient to be relatively
small.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># setting the seed for reproducibility
seed_everything(42, workers=True)

model = FeedForward(
        layers=[10, 10],
        func=torch.nn.Tanh,
        output_dimensions=len(problem.output_variables),
        input_dimensions=len(problem.input_variables)
    )
pinn = PINN(problem, model)
trainer = Trainer(solver=pinn,
                  accelerator=&#39;cpu&#39;,
                  max_epochs = 2000,
                  enable_model_summary=False,
                  gradient_clip_val=0.1,          # clipping the gradient
                  callbacks=[Timer(),
                             StochasticWeightAveraging(swa_lrs=0.005)])
trainer.train()
print(f&#39;Total training time {trainer.callbacks[0].time_elapsed(&quot;train&quot;):.5f} s&#39;)
</pre></div>
</div>
<pre class="literal-block">Seed set to 42
GPU available: True (mps), used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs

Epoch 1598: 100%|██████████| 1/1 [00:00&lt;00:00, 261.80it/s, v_num=46, x0_loss=9e-8, D_loss=2.39e-5, mean_loss=1.2e-5]
Swapping scheduler <cite>ConstantLR</cite> for <cite>SWALR</cite>
<cite>Trainer.fit</cite> stopped: <cite>max_epochs=2000</cite> reached.
Epoch 1999: 100%|██████████| 1/1 [00:00&lt;00:00, 148.99it/s, v_num=46, x0_loss=7.08e-7, D_loss=1.77e-5, mean_loss=9.19e-6]
Total training time 17.01149 s</pre>
<p>As we can see we by applying gradient clipping we were able to even
obtain lower error!</p>
</div>
<div class="section" id="whats-next">
<h2>What’s next?<a class="headerlink" href="#whats-next" title="Permalink to this headline">¶</a></h2>
<p>Now you know how to use efficiently the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> class <strong>PINA</strong>!
There are multiple directions you can go now:</p>
<ol class="arabic simple">
<li><p>Explore training times on different devices (e.g.) <code class="docutils literal notranslate"><span class="pre">TPU</span></code></p></li>
<li><p>Try to reduce memory cost by mixed precision training and gradient
accumulation (especially useful when training Neural Operators)</p></li>
<li><p>Benchmark <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> speed for different precisions.</p></li>
</ol>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../tutorial12/tutorial.html" class="btn btn-neutral float-left" title="Tutorial: The Equation Class" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../tutorial6/tutorial.html" class="btn btn-neutral float-right" title="Tutorial: Building custom geometries with PINA Location class" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021-2024, PINA Contributors.
      <span class="lastupdated">Last updated on May 02, 2024.
      </span></p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>