<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Tutorial: Two dimensional Poisson problem using Extra Features Learning &mdash; PINA 0.1 documentation</title><link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/language_data.js"></script>
        <script src="../../../_static/clipboard.min.js"></script>
        <script src="../../../_static/copybutton.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Tutorial: Two dimensional Wave problem with hard constraint" href="../tutorial3/tutorial.html" />
    <link rel="prev" title="Tutorial: Building custom geometries with PINA Location class" href="../tutorial6/tutorial.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html">
            
              <img src="../../../_static/pina_logo.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                0.1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Package Documentation:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../_code.html">API</a></li>
</ul>
<p class="caption"><span class="caption-text">Getting Started:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../_installation.html">Installation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../_tutorial.html">Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../_tutorial.html#getting-started-with-pina">Getting started with PINA</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../_tutorial.html#physics-informed-neural-networks">Physics Informed Neural Networks</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Two dimensional Poisson problem using Extra Features Learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#the-problem-definition">The problem definition</a></li>
<li class="toctree-l4"><a class="reference internal" href="#solving-the-problem-with-standard-pinns">Solving the problem with standard PINNs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#solving-the-problem-with-extra-features-pinns">Solving the problem with extra-features PINNs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#solving-the-problem-with-learnable-extra-features-pinns">Solving the problem with learnable extra-features PINNs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#whats-next">What’s next?</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial3/tutorial.html">Two dimensional Wave problem with hard constraint</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial7/tutorial.html">Resolution of a 2D Poisson inverse problem</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../_tutorial.html#neural-operator-learning">Neural Operator Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_tutorial.html#supervised-learning">Supervised Learning</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Community:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../_team.html">Team &amp; Foundings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../_LICENSE.html">License</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../_cite.html">Cite PINA</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">PINA</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../_tutorial.html">PINA Tutorials</a></li>
      <li class="breadcrumb-item active">Tutorial: Two dimensional Poisson problem using Extra Features Learning</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/_rst/tutorials/tutorial2/tutorial.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="tutorial-two-dimensional-poisson-problem-using-extra-features-learning">
<h1>Tutorial: Two dimensional Poisson problem using Extra Features Learning<a class="headerlink" href="#tutorial-two-dimensional-poisson-problem-using-extra-features-learning" title="Permalink to this headline">¶</a></h1>
<p>This tutorial presents how to solve with Physics-Informed Neural
Networks (PINNs) a 2D Poisson problem with Dirichlet boundary
conditions. We will train with standard PINN’s training, and with
extrafeatures. For more insights on extrafeature learning please read
<a class="reference external" href="https://www.sciencedirect.com/science/article/abs/pii/S0898122123002018">An extended physics informed neural network for preliminary analysis of
parametric optimal control
problems</a>.</p>
<p>First of all, some useful imports.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import torch
from torch.nn import Softplus

from pina.problem import SpatialProblem
from pina.operators import laplacian
from pina.model import FeedForward
from pina.solvers import PINN
from pina.trainer import Trainer
from pina.plotter import Plotter
from pina.geometry import CartesianDomain
from pina.equation import Equation, FixedValue
from pina import Condition, LabelTensor
from pina.callbacks import MetricTracker
</pre></div>
</div>
<div class="section" id="the-problem-definition">
<h2>The problem definition<a class="headerlink" href="#the-problem-definition" title="Permalink to this headline">¶</a></h2>
<p>The two-dimensional Poisson problem is mathematically written as:
<a href="#id1"><span class="problematic" id="id2">:raw-latex:`\begin{equation}
\begin{cases}
\Delta u = \sin{(\pi x)} \sin{(\pi y)} \text{ in } D, \\
u = 0 \text{ on } \Gamma_1 \cup \Gamma_2 \cup \Gamma_3 \cup \Gamma_4,
\end{cases}
\end{equation}`</span></a> where <span class="math notranslate nohighlight">\(D\)</span> is a square domain <span class="math notranslate nohighlight">\([0,1]^2\)</span>, and
<span class="math notranslate nohighlight">\(\Gamma_i\)</span>, with <span class="math notranslate nohighlight">\(i=1,...,4\)</span>, are the boundaries of the
square.</p>
<p>The Poisson problem is written in <strong>PINA</strong> code as a class. The
equations are written as <em>conditions</em> that should be satisfied in the
corresponding domains. The <em>truth_solution</em> is the exact solution which
will be compared with the predicted one.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class Poisson(SpatialProblem):
    output_variables = [&#39;u&#39;]
    spatial_domain = CartesianDomain({&#39;x&#39;: [0, 1], &#39;y&#39;: [0, 1]})

    def laplace_equation(input_, output_):
        force_term = (torch.sin(input_.extract([&#39;x&#39;])*torch.pi) *
                      torch.sin(input_.extract([&#39;y&#39;])*torch.pi))
        laplacian_u = laplacian(output_, input_, components=[&#39;u&#39;], d=[&#39;x&#39;, &#39;y&#39;])
        return laplacian_u - force_term

    # here we write the problem conditions
    conditions = {
        &#39;gamma1&#39;: Condition(location=CartesianDomain({&#39;x&#39;: [0, 1], &#39;y&#39;:  1}), equation=FixedValue(0.)),
        &#39;gamma2&#39;: Condition(location=CartesianDomain({&#39;x&#39;: [0, 1], &#39;y&#39;: 0}), equation=FixedValue(0.)),
        &#39;gamma3&#39;: Condition(location=CartesianDomain({&#39;x&#39;:  1, &#39;y&#39;: [0, 1]}), equation=FixedValue(0.)),
        &#39;gamma4&#39;: Condition(location=CartesianDomain({&#39;x&#39;: 0, &#39;y&#39;: [0, 1]}), equation=FixedValue(0.)),
        &#39;D&#39;: Condition(location=CartesianDomain({&#39;x&#39;: [0, 1], &#39;y&#39;: [0, 1]}), equation=Equation(laplace_equation)),
    }

    def poisson_sol(self, pts):
        return -(
            torch.sin(pts.extract([&#39;x&#39;])*torch.pi)*
            torch.sin(pts.extract([&#39;y&#39;])*torch.pi)
        )/(2*torch.pi**2)

    truth_solution = poisson_sol

problem = Poisson()

# let&#39;s discretise the domain
problem.discretise_domain(25, &#39;grid&#39;, locations=[&#39;D&#39;])
problem.discretise_domain(25, &#39;grid&#39;, locations=[&#39;gamma1&#39;, &#39;gamma2&#39;, &#39;gamma3&#39;, &#39;gamma4&#39;])
</pre></div>
</div>
</div>
<div class="section" id="solving-the-problem-with-standard-pinns">
<h2>Solving the problem with standard PINNs<a class="headerlink" href="#solving-the-problem-with-standard-pinns" title="Permalink to this headline">¶</a></h2>
<p>After the problem, the feed-forward neural network is defined, through
the class <code class="docutils literal notranslate"><span class="pre">FeedForward</span></code>. This neural network takes as input the
coordinates (in this case <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>) and provides the
unkwown field of the Poisson problem. The residual of the equations are
evaluated at several sampling points (which the user can manipulate
using the method <code class="docutils literal notranslate"><span class="pre">CartesianDomain_pts</span></code>) and the loss minimized by the
neural network is the sum of the residuals.</p>
<p>In this tutorial, the neural network is composed by two hidden layers of
10 neurons each, and it is trained for 1000 epochs with a learning rate
of 0.006 and <span class="math notranslate nohighlight">\(l_2\)</span> weight regularization set to <span class="math notranslate nohighlight">\(10^{-7}\)</span>.
These parameters can be modified as desired. We use the
<code class="docutils literal notranslate"><span class="pre">MetricTracker</span></code> class to track the metrics during training.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># make model + solver + trainer
model = FeedForward(
    layers=[10, 10],
    func=Softplus,
    output_dimensions=len(problem.output_variables),
    input_dimensions=len(problem.input_variables)
)
pinn = PINN(problem, model, optimizer_kwargs={&#39;lr&#39;:0.006, &#39;weight_decay&#39;:1e-8})
trainer = Trainer(pinn, max_epochs=1000, callbacks=[MetricTracker()], accelerator=&#39;cpu&#39;, enable_model_summary=False) # we train on CPU and avoid model summary at beginning of training (optional)

# train
trainer.train()
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">GPU</span> <span class="n">available</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="n">used</span><span class="p">:</span> <span class="kc">False</span>
<span class="n">TPU</span> <span class="n">available</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="n">using</span><span class="p">:</span> <span class="mi">0</span> <span class="n">TPU</span> <span class="n">cores</span>
<span class="n">IPU</span> <span class="n">available</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="n">using</span><span class="p">:</span> <span class="mi">0</span> <span class="n">IPUs</span>
<span class="n">HPU</span> <span class="n">available</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="n">using</span><span class="p">:</span> <span class="mi">0</span> <span class="n">HPUs</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Epoch</span> <span class="mi">999</span><span class="p">:</span> <span class="p">:</span> <span class="mi">1</span><span class="n">it</span> <span class="p">[</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">,</span> <span class="mf">158.53</span><span class="n">it</span><span class="o">/</span><span class="n">s</span><span class="p">,</span> <span class="n">v_num</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">gamma1_loss</span><span class="o">=</span><span class="mf">5.29e-5</span><span class="p">,</span> <span class="n">gamma2_loss</span><span class="o">=</span><span class="mf">4.09e-5</span><span class="p">,</span> <span class="n">gamma3_loss</span><span class="o">=</span><span class="mf">4.73e-5</span><span class="p">,</span> <span class="n">gamma4_loss</span><span class="o">=</span><span class="mf">4.18e-5</span><span class="p">,</span> <span class="n">D_loss</span><span class="o">=</span><span class="mf">0.00134</span><span class="p">,</span> <span class="n">mean_loss</span><span class="o">=</span><span class="mf">0.000304</span><span class="p">]</span>
</pre></div>
</div>
<pre class="literal-block"><cite>Trainer.fit</cite> stopped: <cite>max_epochs=1000</cite> reached.</pre>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Epoch</span> <span class="mi">999</span><span class="p">:</span> <span class="p">:</span> <span class="mi">1</span><span class="n">it</span> <span class="p">[</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">,</span> <span class="mf">105.33</span><span class="n">it</span><span class="o">/</span><span class="n">s</span><span class="p">,</span> <span class="n">v_num</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">gamma1_loss</span><span class="o">=</span><span class="mf">5.29e-5</span><span class="p">,</span> <span class="n">gamma2_loss</span><span class="o">=</span><span class="mf">4.09e-5</span><span class="p">,</span> <span class="n">gamma3_loss</span><span class="o">=</span><span class="mf">4.73e-5</span><span class="p">,</span> <span class="n">gamma4_loss</span><span class="o">=</span><span class="mf">4.18e-5</span><span class="p">,</span> <span class="n">D_loss</span><span class="o">=</span><span class="mf">0.00134</span><span class="p">,</span> <span class="n">mean_loss</span><span class="o">=</span><span class="mf">0.000304</span><span class="p">]</span>
</pre></div>
</div>
<p>Now the <code class="docutils literal notranslate"><span class="pre">Plotter</span></code> class is used to plot the results. The solution
predicted by the neural network is plotted on the left, the exact one is
represented at the center and on the right the error between the exact
and the predicted solutions is showed.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plotter = Plotter()
plotter.plot(solver=pinn)
</pre></div>
</div>
<img alt="../../../_images/tutorial_9_0.png" src="../../../_images/tutorial_9_0.png" />
</div>
<div class="section" id="solving-the-problem-with-extra-features-pinns">
<h2>Solving the problem with extra-features PINNs<a class="headerlink" href="#solving-the-problem-with-extra-features-pinns" title="Permalink to this headline">¶</a></h2>
<p>Now, the same problem is solved in a different way. A new neural network
is now defined, with an additional input variable, named extra-feature,
which coincides with the forcing term in the Laplace equation. The set
of input variables to the neural network is:</p>
<p><a href="#id3"><span class="problematic" id="id4">:raw-latex:`\begin{equation}
[x, y, k(x, y)], \text{ with } k(x, y)=\sin{(\pi x)}\sin{(\pi y)},
\end{equation}`</span></a></p>
<p>where <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are the spatial coordinates and
<span class="math notranslate nohighlight">\(k(x, y)\)</span> is the added feature.</p>
<p>This feature is initialized in the class <code class="docutils literal notranslate"><span class="pre">SinSin</span></code>, which needs to be
inherited by the <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> class and to have the <code class="docutils literal notranslate"><span class="pre">forward</span></code>
method. After declaring such feature, we can just incorporate in the
<code class="docutils literal notranslate"><span class="pre">FeedForward</span></code> class thanks to the <code class="docutils literal notranslate"><span class="pre">extra_features</span></code> argument. <strong>NB</strong>:
<code class="docutils literal notranslate"><span class="pre">extra_features</span></code> always needs a <code class="docutils literal notranslate"><span class="pre">list</span></code> as input, you you have one
feature just encapsulated it in a class, as in the next cell.</p>
<p>Finally, we perform the same training as before: the problem is
<code class="docutils literal notranslate"><span class="pre">Poisson</span></code>, the network is composed by the same number of neurons and
optimizer parameters are equal to previous test, the only change is the
new extra feature.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class SinSin(torch.nn.Module):
    &quot;&quot;&quot;Feature: sin(x)*sin(y)&quot;&quot;&quot;
    def __init__(self):
        super().__init__()

    def forward(self, x):
        t = (torch.sin(x.extract([&#39;x&#39;])*torch.pi) *
             torch.sin(x.extract([&#39;y&#39;])*torch.pi))
        return LabelTensor(t, [&#39;sin(x)sin(y)&#39;])


# make model + solver + trainer
model_feat = FeedForward(
    layers=[10, 10],
    func=Softplus,
    output_dimensions=len(problem.output_variables),
    input_dimensions=len(problem.input_variables)+1
)
pinn_feat = PINN(problem, model_feat, extra_features=[SinSin()], optimizer_kwargs={&#39;lr&#39;:0.006, &#39;weight_decay&#39;:1e-8})
trainer_feat = Trainer(pinn_feat, max_epochs=1000, callbacks=[MetricTracker()], accelerator=&#39;cpu&#39;, enable_model_summary=False) # we train on CPU and avoid model summary at beginning of training (optional)

# train
trainer_feat.train()
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">GPU</span> <span class="n">available</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="n">used</span><span class="p">:</span> <span class="kc">False</span>
<span class="n">TPU</span> <span class="n">available</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="n">using</span><span class="p">:</span> <span class="mi">0</span> <span class="n">TPU</span> <span class="n">cores</span>
<span class="n">IPU</span> <span class="n">available</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="n">using</span><span class="p">:</span> <span class="mi">0</span> <span class="n">IPUs</span>
<span class="n">HPU</span> <span class="n">available</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="n">using</span><span class="p">:</span> <span class="mi">0</span> <span class="n">HPUs</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Epoch</span> <span class="mi">999</span><span class="p">:</span> <span class="p">:</span> <span class="mi">1</span><span class="n">it</span> <span class="p">[</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">,</span> <span class="mf">111.88</span><span class="n">it</span><span class="o">/</span><span class="n">s</span><span class="p">,</span> <span class="n">v_num</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">gamma1_loss</span><span class="o">=</span><span class="mf">2.54e-7</span><span class="p">,</span> <span class="n">gamma2_loss</span><span class="o">=</span><span class="mf">2.17e-7</span><span class="p">,</span> <span class="n">gamma3_loss</span><span class="o">=</span><span class="mf">1.94e-7</span><span class="p">,</span> <span class="n">gamma4_loss</span><span class="o">=</span><span class="mf">2.69e-7</span><span class="p">,</span> <span class="n">D_loss</span><span class="o">=</span><span class="mf">9.2e-6</span><span class="p">,</span> <span class="n">mean_loss</span><span class="o">=</span><span class="mf">2.03e-6</span><span class="p">]</span>
</pre></div>
</div>
<pre class="literal-block"><cite>Trainer.fit</cite> stopped: <cite>max_epochs=1000</cite> reached.</pre>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Epoch</span> <span class="mi">999</span><span class="p">:</span> <span class="p">:</span> <span class="mi">1</span><span class="n">it</span> <span class="p">[</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">,</span> <span class="mf">85.62</span><span class="n">it</span><span class="o">/</span><span class="n">s</span><span class="p">,</span> <span class="n">v_num</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">gamma1_loss</span><span class="o">=</span><span class="mf">2.54e-7</span><span class="p">,</span> <span class="n">gamma2_loss</span><span class="o">=</span><span class="mf">2.17e-7</span><span class="p">,</span> <span class="n">gamma3_loss</span><span class="o">=</span><span class="mf">1.94e-7</span><span class="p">,</span> <span class="n">gamma4_loss</span><span class="o">=</span><span class="mf">2.69e-7</span><span class="p">,</span> <span class="n">D_loss</span><span class="o">=</span><span class="mf">9.2e-6</span><span class="p">,</span> <span class="n">mean_loss</span><span class="o">=</span><span class="mf">2.03e-6</span><span class="p">]</span>
</pre></div>
</div>
<p>The predicted and exact solutions and the error between them are
represented below. We can easily note that now our network, having
almost the same condition as before, is able to reach additional order
of magnitudes in accuracy.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plotter.plot(solver=pinn_feat)
</pre></div>
</div>
<img alt="../../../_images/tutorial_14_0.png" src="../../../_images/tutorial_14_0.png" />
</div>
<div class="section" id="solving-the-problem-with-learnable-extra-features-pinns">
<h2>Solving the problem with learnable extra-features PINNs<a class="headerlink" href="#solving-the-problem-with-learnable-extra-features-pinns" title="Permalink to this headline">¶</a></h2>
<p>We can still do better!</p>
<p>Another way to exploit the extra features is the addition of learnable
parameter inside them. In this way, the added parameters are learned
during the training phase of the neural network. In this case, we use:</p>
<p><a href="#id5"><span class="problematic" id="id6">:raw-latex:`\begin{equation}
k(x, \mathbf{y}) = \beta \sin{(\alpha x)} \sin{(\alpha y)},
\end{equation}`</span></a></p>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are the abovementioned
parameters. Their implementation is quite trivial: by using the class
<code class="docutils literal notranslate"><span class="pre">torch.nn.Parameter</span></code> we cam define all the learnable parameters we
need, and they are managed by <code class="docutils literal notranslate"><span class="pre">autograd</span></code> module!</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class SinSinAB(torch.nn.Module):
    &quot;&quot;&quot; &quot;&quot;&quot;
    def __init__(self):
        super().__init__()
        self.alpha = torch.nn.Parameter(torch.tensor([1.0]))
        self.beta = torch.nn.Parameter(torch.tensor([1.0]))


    def forward(self, x):
        t =  (
            self.beta*torch.sin(self.alpha*x.extract([&#39;x&#39;])*torch.pi)*
                      torch.sin(self.alpha*x.extract([&#39;y&#39;])*torch.pi)
        )
        return LabelTensor(t, [&#39;b*sin(a*x)sin(a*y)&#39;])


# make model + solver + trainer
model_lean= FeedForward(
    layers=[10, 10],
    func=Softplus,
    output_dimensions=len(problem.output_variables),
    input_dimensions=len(problem.input_variables)+1
)
pinn_lean = PINN(problem, model_lean, extra_features=[SinSinAB()], optimizer_kwargs={&#39;lr&#39;:0.006, &#39;weight_decay&#39;:1e-8})
trainer_learn = Trainer(pinn_lean, max_epochs=1000, accelerator=&#39;cpu&#39;, enable_model_summary=False) # we train on CPU and avoid model summary at beginning of training (optional)

# train
trainer_learn.train()
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">GPU</span> <span class="n">available</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="n">used</span><span class="p">:</span> <span class="kc">False</span>
<span class="n">TPU</span> <span class="n">available</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="n">using</span><span class="p">:</span> <span class="mi">0</span> <span class="n">TPU</span> <span class="n">cores</span>
<span class="n">IPU</span> <span class="n">available</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="n">using</span><span class="p">:</span> <span class="mi">0</span> <span class="n">IPUs</span>
<span class="n">HPU</span> <span class="n">available</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="n">using</span><span class="p">:</span> <span class="mi">0</span> <span class="n">HPUs</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Epoch</span> <span class="mi">999</span><span class="p">:</span> <span class="p">:</span> <span class="mi">1</span><span class="n">it</span> <span class="p">[</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">,</span> <span class="mf">119.29</span><span class="n">it</span><span class="o">/</span><span class="n">s</span><span class="p">,</span> <span class="n">v_num</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">gamma1_loss</span><span class="o">=</span><span class="mf">3.26e-8</span><span class="p">,</span> <span class="n">gamma2_loss</span><span class="o">=</span><span class="mf">7.84e-8</span><span class="p">,</span> <span class="n">gamma3_loss</span><span class="o">=</span><span class="mf">1.13e-7</span><span class="p">,</span> <span class="n">gamma4_loss</span><span class="o">=</span><span class="mf">3.02e-8</span><span class="p">,</span> <span class="n">D_loss</span><span class="o">=</span><span class="mf">2.66e-6</span><span class="p">,</span> <span class="n">mean_loss</span><span class="o">=</span><span class="mf">5.82e-7</span><span class="p">]</span>
</pre></div>
</div>
<pre class="literal-block"><cite>Trainer.fit</cite> stopped: <cite>max_epochs=1000</cite> reached.</pre>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Epoch</span> <span class="mi">999</span><span class="p">:</span> <span class="p">:</span> <span class="mi">1</span><span class="n">it</span> <span class="p">[</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">,</span> <span class="mf">85.94</span><span class="n">it</span><span class="o">/</span><span class="n">s</span><span class="p">,</span> <span class="n">v_num</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">gamma1_loss</span><span class="o">=</span><span class="mf">3.26e-8</span><span class="p">,</span> <span class="n">gamma2_loss</span><span class="o">=</span><span class="mf">7.84e-8</span><span class="p">,</span> <span class="n">gamma3_loss</span><span class="o">=</span><span class="mf">1.13e-7</span><span class="p">,</span> <span class="n">gamma4_loss</span><span class="o">=</span><span class="mf">3.02e-8</span><span class="p">,</span> <span class="n">D_loss</span><span class="o">=</span><span class="mf">2.66e-6</span><span class="p">,</span> <span class="n">mean_loss</span><span class="o">=</span><span class="mf">5.82e-7</span><span class="p">]</span>
</pre></div>
</div>
<p>Umh, the final loss is not appreciabily better than previous model (with
static extra features), despite the usage of learnable parameters. This
is mainly due to the over-parametrization of the network: there are many
parameter to optimize during the training, and the model in unable to
understand automatically that only the parameters of the extra feature
(and not the weights/bias of the FFN) should be tuned in order to fit
our problem. A longer training can be helpful, but in this case the
faster way to reach machine precision for solving the Poisson problem is
removing all the hidden layers in the <code class="docutils literal notranslate"><span class="pre">FeedForward</span></code>, keeping only the
<span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> parameters of the extra feature.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># make model + solver + trainer
model_lean= FeedForward(
    layers=[],
    func=Softplus,
    output_dimensions=len(problem.output_variables),
    input_dimensions=len(problem.input_variables)+1
)
pinn_learn = PINN(problem, model_lean, extra_features=[SinSinAB()], optimizer_kwargs={&#39;lr&#39;:0.01, &#39;weight_decay&#39;:1e-8})
trainer_learn = Trainer(pinn_learn, max_epochs=1000, callbacks=[MetricTracker()], accelerator=&#39;cpu&#39;, enable_model_summary=False) # we train on CPU and avoid model summary at beginning of training (optional)

# train
trainer_learn.train()
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">GPU</span> <span class="n">available</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="n">used</span><span class="p">:</span> <span class="kc">False</span>
<span class="n">TPU</span> <span class="n">available</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="n">using</span><span class="p">:</span> <span class="mi">0</span> <span class="n">TPU</span> <span class="n">cores</span>
<span class="n">IPU</span> <span class="n">available</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="n">using</span><span class="p">:</span> <span class="mi">0</span> <span class="n">IPUs</span>
<span class="n">HPU</span> <span class="n">available</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="n">using</span><span class="p">:</span> <span class="mi">0</span> <span class="n">HPUs</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Epoch 0: : 0it [00:00, ?it/s]Epoch 999: : 1it [00:00, 131.20it/s, v_num=6, gamma1_loss=2.55e-16, gamma2_loss=4.76e-17, gamma3_loss=2.55e-16, gamma4_loss=4.76e-17, D_loss=1.74e-13, mean_loss=3.5e-14]
</pre></div>
</div>
<pre class="literal-block"><cite>Trainer.fit</cite> stopped: <cite>max_epochs=1000</cite> reached.</pre>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Epoch</span> <span class="mi">999</span><span class="p">:</span> <span class="p">:</span> <span class="mi">1</span><span class="n">it</span> <span class="p">[</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">,</span> <span class="mf">98.81</span><span class="n">it</span><span class="o">/</span><span class="n">s</span><span class="p">,</span> <span class="n">v_num</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">gamma1_loss</span><span class="o">=</span><span class="mf">2.55e-16</span><span class="p">,</span> <span class="n">gamma2_loss</span><span class="o">=</span><span class="mf">4.76e-17</span><span class="p">,</span> <span class="n">gamma3_loss</span><span class="o">=</span><span class="mf">2.55e-16</span><span class="p">,</span> <span class="n">gamma4_loss</span><span class="o">=</span><span class="mf">4.76e-17</span><span class="p">,</span> <span class="n">D_loss</span><span class="o">=</span><span class="mf">1.74e-13</span><span class="p">,</span> <span class="n">mean_loss</span><span class="o">=</span><span class="mf">3.5e-14</span><span class="p">]</span>
</pre></div>
</div>
<p>In such a way, the model is able to reach a very high accuracy! Of
course, this is a toy problem for understanding the usage of extra
features: similar precision could be obtained if the extra features are
very similar to the true solution. The analyzed Poisson problem shows a
forcing term very close to the solution, resulting in a perfect problem
to address with such an approach.</p>
<p>We conclude here by showing the graphical comparison of the unknown
field and the loss trend for all the test cases presented here: the
standard PINN, PINN with extra features, and PINN with learnable extra
features.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plotter.plot(solver=pinn_learn)
</pre></div>
</div>
<img alt="../../../_images/tutorial_21_0.png" src="../../../_images/tutorial_21_0.png" />
<p>Let us compare the training losses for the various types of training</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plotter.plot_loss(trainer, logy=True, label=&#39;Standard&#39;)
plotter.plot_loss(trainer_feat, logy=True,label=&#39;Static Features&#39;)
plotter.plot_loss(trainer_learn, logy=True, label=&#39;Learnable Features&#39;)
</pre></div>
</div>
<img alt="../../../_images/tutorial_23_01.png" src="../../../_images/tutorial_23_01.png" />
</div>
<div class="section" id="whats-next">
<h2>What’s next?<a class="headerlink" href="#whats-next" title="Permalink to this headline">¶</a></h2>
<p>Nice you have completed the two dimensional Poisson tutorial of
<strong>PINA</strong>! There are multiple directions you can go now:</p>
<ol class="arabic simple">
<li><p>Train the network for longer or with different layer sizes and assert
the finaly accuracy</p></li>
<li><p>Propose new types of extrafeatures and see how they affect the
learning</p></li>
<li><p>Exploit extrafeature training in more complex problems</p></li>
<li><p>Many more…</p></li>
</ol>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../tutorial6/tutorial.html" class="btn btn-neutral float-left" title="Tutorial: Building custom geometries with PINA Location class" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../tutorial3/tutorial.html" class="btn btn-neutral float-right" title="Tutorial: Two dimensional Wave problem with hard constraint" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Copyright 2021-2023, PINA Contributors.
      <span class="lastupdated">Last updated on Nov 17, 2023.
      </span></p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>