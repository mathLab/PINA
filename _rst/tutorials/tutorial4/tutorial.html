
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Tutorial: Unstructured convolutional autoencoder via continuous convolution &#8212; PINA 0.1.2.post2501 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="/css/custom.css" />
  
  <!-- So that users can add custom icons -->
  <script src="../../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../../_static/documentation_options.js?v=ad75e52c"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_rst/tutorials/tutorial4/tutorial';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Tutorial: Reduced order model (POD-RBF or POD-NN) for parametric problems" href="../tutorial8/tutorial.html" />
    <link rel="prev" title="Tutorial: Averaging Neural Operator for solving Kuramoto Sivashinsky equation" href="../tutorial10/tutorial.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="0.1.2.post2501" />
    <meta name="docbuild:last-update" content="Jan 01, 2025"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../_static/PINA_logo.png" class="logo__image only-light" alt="PINA 0.1.2.post2501 documentation - Home"/>
    <img src="../../../_static/PINA_logo.png" class="logo__image only-dark pst-js-only" alt="PINA 0.1.2.post2501 documentation - Home"/>
  
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../_installation.html">
    Installing
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../../_tutorial.html">
    Tutorial
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../_code.html">
    API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../_team.html">
    Team & Foundings
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../_contributing.html">
    Contributing
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../_LICENSE.html">
    License
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../_cite.html">
    Cite PINA
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/mathLab/PINA" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fab fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/pina_mathlab?s=21" title="Twitter" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Twitter</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="mailto:pina.mathlab@gmail.com" title="Email" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Email</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../_installation.html">
    Installing
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../../_tutorial.html">
    Tutorial
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../_code.html">
    API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../_team.html">
    Team & Foundings
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../_contributing.html">
    Contributing
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../_LICENSE.html">
    License
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../_cite.html">
    Cite PINA
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/mathLab/PINA" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fab fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/pina_mathlab?s=21" title="Twitter" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Twitter</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="mailto:pina.mathlab@gmail.com" title="Email" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Email</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tutorial1/tutorial.html">Introduction to PINA for Physics Informed Neural Networks training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial12/tutorial.html">Introduction to PINA Equation class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial11/tutorial.html">PINA and PyTorch Lightning, training tips and visualizations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial6/tutorial.html">Building custom geometries with PINA Location class</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tutorial2/tutorial.html">Two dimensional Poisson problem using Extra Features Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial3/tutorial.html">Two dimensional Wave problem with hard constraint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial7/tutorial.html">Resolution of a 2D Poisson inverse problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial9/tutorial.html">Periodic Boundary Conditions for Helmotz Equation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial13/tutorial.html">Multiscale PDE learning with Fourier Feature Network</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tutorial5/tutorial.html">Two dimensional Darcy flow using the Fourier Neural Operator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial10/tutorial.html">Time dependent Kuramoto Sivashinsky equation using the Averaging Neural Operator</a></li>
</ul>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Unstructured convolutional autoencoder via continuous convolution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial8/tutorial.html">POD-RBF and POD-NN for reduced order modeling</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../_tutorial.html" class="nav-link">PINA Tutorials</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Tutorial: Unstructured convolutional autoencoder via continuous convolution</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="tutorial-unstructured-convolutional-autoencoder-via-continuous-convolution">
<h1>Tutorial: Unstructured convolutional autoencoder via continuous convolution<a class="headerlink" href="#tutorial-unstructured-convolutional-autoencoder-via-continuous-convolution" title="Link to this heading">#</a></h1>
<p><a class="reference external" href="https://colab.research.google.com/github/mathLab/PINA/blob/master/tutorials/tutorial4/tutorial.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<p>In this tutorial, we will show how to use the Continuous Convolutional
Filter, and how to build common Deep Learning architectures with it. The
implementation of the filter follows the original work <a class="reference external" href="https://arxiv.org/abs/2210.13416">A Continuous
Convolutional Trainable Filter for Modelling Unstructured
Data</a>.</p>
<p>First of all we import the modules needed for the tutorial:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>## routine needed to run the notebook on Google Colab
try:
  import google.colab
  IN_COLAB = True
except:
  IN_COLAB = False
if IN_COLAB:
  !pip install &quot;pina-mathlab&quot;

import torch
import matplotlib.pyplot as plt
plt.style.use(&#39;tableau-colorblind10&#39;)
from pina.problem import AbstractProblem
from pina.solvers import SupervisedSolver
from pina.trainer import Trainer
from pina import Condition, LabelTensor
from pina.model.layers import ContinuousConvBlock
import torchvision # for MNIST dataset
from pina.model import FeedForward # for building AE and MNIST classification
</pre></div>
</div>
<p>The tutorial is structured as follow:</p>
<ul class="simple">
<li><p><a class="reference external" href="#continuous-filter-background">Continuous filter background</a>: understand how the convolutional filter works and how to use it.</p></li>
<li><p><a class="reference external" href="#building-a-mnist-classifier">Building a MNIST Classifier</a>: show how to build a simple
classifier using the MNIST dataset and how to combine a continuous
convolutional layer with a feedforward neural network.</p></li>
<li><p><a class="reference external" href="#building-a-continuous-convolutional-autoencoder">Building a Continuous Convolutional Autoencoder</a>: show
show to use the continuous filter to work with unstructured data for
autoencoding and up-sampling.</p></li>
</ul>
<section id="continuous-filter-background">
<h2>Continuous filter background<a class="headerlink" href="#continuous-filter-background" title="Link to this heading">#</a></h2>
<p>As reported by the authors in the original paper: in contrast to
discrete convolution, continuous convolution is mathematically defined
as:</p>
<div class="math notranslate nohighlight">
\[\mathcal{I}_{\rm{out}}(\mathbf{x}) = \int_{\mathcal{X}}  \mathcal{I}(\mathbf{x} + \mathbf{\tau}) \cdot \mathcal{K}(\mathbf{\tau}) d\mathbf{\tau},\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{K} : \mathcal{X} \rightarrow \mathbb{R}\)</span> is the
<em>continuous filter</em> function, and
<span class="math notranslate nohighlight">\(\mathcal{I} : \Omega \subset \mathbb{R}^N \rightarrow \mathbb{R}\)</span>
is the input function. The continuous filter function is approximated
using a FeedForward Neural Network, thus trainable during the training
phase. The way in which the integral is approximated can be different,
currently on <strong>PINA</strong> we approximate it using a simple sum, as suggested
by the authors. Thus, given <span class="math notranslate nohighlight">\(\{\mathbf{x}_i\}_{i=1}^{n}\)</span> points in
<span class="math notranslate nohighlight">\(\mathbb{R}^N\)</span> of the input function mapped on the
<span class="math notranslate nohighlight">\(\mathcal{X}\)</span> filter domain, we approximate the above equation as:</p>
<div class="math notranslate nohighlight">
\[\mathcal{I}_{\rm{out}}(\mathbf{\tilde{x}}_i) = \sum_{{\mathbf{x}_i}\in\mathcal{X}}  \mathcal{I}(\mathbf{x}_i + \mathbf{\tau}) \cdot \mathcal{K}(\mathbf{x}_i),\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{\tau} \in \mathcal{S}\)</span>, with <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>
the set of available strides, corresponds to the current stride position
of the filter, and <span class="math notranslate nohighlight">\(\mathbf{\tilde{x}}_i\)</span> points are obtained by
taking the centroid of the filter position mapped on the <span class="math notranslate nohighlight">\(\Omega\)</span>
domain.</p>
<p>We will now try to pratically see how to work with the filter. From the
above definition we see that what is needed is: 1. A domain and a
function defined on that domain (the input) 2. A stride, corresponding
to the positions where the filter needs to be <span class="math notranslate nohighlight">\(\rightarrow\)</span>
<code class="docutils literal notranslate"><span class="pre">stride</span></code> variable in <code class="docutils literal notranslate"><span class="pre">ContinuousConv</span></code> 3. The filter rectangular
domain <span class="math notranslate nohighlight">\(\rightarrow\)</span> <code class="docutils literal notranslate"><span class="pre">filter_dim</span></code> variable in <code class="docutils literal notranslate"><span class="pre">ContinuousConv</span></code></p>
<section id="input-function">
<h3>Input function<a class="headerlink" href="#input-function" title="Link to this heading">#</a></h3>
<p>The input function for the continuous filter is defined as a tensor of
shape:</p>
<div class="math notranslate nohighlight">
\[[B \times N_{in} \times N \times D]\]</div>
<p>where <span class="math notranslate nohighlight">\(B\)</span> is the batch_size, <span class="math notranslate nohighlight">\(N_{in}\)</span> is the number of
input fields, <span class="math notranslate nohighlight">\(N\)</span> the number of points in the mesh, <span class="math notranslate nohighlight">\(D\)</span> the
dimension of the problem. In particular: * <span class="math notranslate nohighlight">\(D\)</span> is the number of
spatial variables + 1. The last column must contain the field value. For
example for 2D problems <span class="math notranslate nohighlight">\(D=3\)</span> and the tensor will be something
like <code class="docutils literal notranslate"><span class="pre">[first</span> <span class="pre">coordinate,</span> <span class="pre">second</span> <span class="pre">coordinate,</span> <span class="pre">field</span> <span class="pre">value]</span></code> *
<span class="math notranslate nohighlight">\(N_{in}\)</span> represents the number of vectorial function presented.
For example a vectorial function <span class="math notranslate nohighlight">\(f = [f_1, f_2]\)</span> will have
<span class="math notranslate nohighlight">\(N_{in}=2\)</span></p>
<p>Let’s see an example to clear the ideas. We will be verbose to explain
in details the input form. We wish to create the function:</p>
<div class="math notranslate nohighlight">
\[f(x, y) = [\sin(\pi x) \sin(\pi y), -\sin(\pi x) \sin(\pi y)] \quad (x,y)\in[0,1]\times[0,1]\]</div>
<p>using a batch size of one.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># batch size fixed to 1
batch_size = 1

# points in the mesh fixed to 200
N = 200

# vectorial 2 dimensional function, number_input_fileds=2
number_input_fileds = 2

# 2 dimensional spatial variables, D = 2 + 1 = 3
D = 3

# create the function f domain as random 2d points in [0, 1]
domain = torch.rand(size=(batch_size, number_input_fileds, N, D-1))
print(f&quot;Domain has shape: {domain.shape}&quot;)

# create the functions
pi = torch.acos(torch.tensor([-1.])) # pi value
f1 = torch.sin(pi * domain[:, 0, :, 0]) * torch.sin(pi * domain[:, 0, :, 1])
f2 = - torch.sin(pi * domain[:, 1, :, 0]) * torch.sin(pi * domain[:, 1, :, 1])

# stacking the input domain and field values
data = torch.empty(size=(batch_size, number_input_fileds, N, D))
data[..., :-1] = domain # copy the domain
data[:, 0, :, -1] = f1 # copy first field value
data[:, 1, :, -1] = f1  # copy second field value
print(f&quot;Filter input data has shape: {data.shape}&quot;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Domain</span> <span class="n">has</span> <span class="n">shape</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">Filter</span> <span class="nb">input</span> <span class="n">data</span> <span class="n">has</span> <span class="n">shape</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="stride">
<h3>Stride<a class="headerlink" href="#stride" title="Link to this heading">#</a></h3>
<p>The stride is passed as a dictionary <code class="docutils literal notranslate"><span class="pre">stride</span></code> which tells the filter
where to go. Here is an example for the <span class="math notranslate nohighlight">\([0,1]\times[0,5]\)</span> domain:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># stride definition</span>
<span class="n">stride</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;domain&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
          <span class="s2">&quot;start&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
          <span class="s2">&quot;jump&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
          <span class="s2">&quot;direction&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
          <span class="p">}</span>
</pre></div>
</div>
<p>This tells the filter:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">domain</span></code>: square domain (the only implemented) <span class="math notranslate nohighlight">\([0,1]\times[0,5]\)</span>. The minimum value is always zero,
while the maximum is specified by the user</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">start</span></code>: start position
of the filter, coordinate <span class="math notranslate nohighlight">\((0, 0)\)</span></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">jump</span></code>: the jumps of the
centroid of the filter to the next position <span class="math notranslate nohighlight">\((0.1, 0.3)\)</span></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">direction</span></code>: the directions of the jump, with <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">=</span> <span class="pre">right</span></code>,
<code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">=</span> <span class="pre">no</span> <span class="pre">jump</span></code>,<code class="docutils literal notranslate"><span class="pre">-1</span> <span class="pre">=</span> <span class="pre">left</span></code> with respect to the current position</p></li>
</ol>
<p><strong>Note</strong></p>
<p>We are planning to release the possibility to directly pass a list of
possible strides!</p>
</section>
<section id="filter-definition">
<h3>Filter definition<a class="headerlink" href="#filter-definition" title="Link to this heading">#</a></h3>
<p>Having defined all the previous blocks we are able to construct the
continuous filter. Suppose we would like to get an ouput with only one field, and let us
fix the filter dimension to be <span class="math notranslate nohighlight">\([0.1, 0.1]\)</span>.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># filter dim
filter_dim = [0.1, 0.1]

# stride
stride = {&quot;domain&quot;: [1, 1],
          &quot;start&quot;: [0, 0],
          &quot;jump&quot;: [0.08, 0.08],
          &quot;direction&quot;: [1, 1],
          }

# creating the filter
cConv = ContinuousConvBlock(input_numb_field=number_input_fileds,
                        output_numb_field=1,
                        filter_dim=filter_dim,
                        stride=stride)
</pre></div>
</div>
<p>That’s it! In just one line of code we have created the continuous
convolutional filter. By default the <code class="docutils literal notranslate"><span class="pre">pina.model.FeedForward</span></code> neural
network is intitialised, more on the
<a class="reference external" href="https://mathlab.github.io/PINA/_rst/fnn.html">documentation</a>. In
case the mesh doesn’t change during training we can set the <code class="docutils literal notranslate"><span class="pre">optimize</span></code>
flag equals to <code class="docutils literal notranslate"><span class="pre">True</span></code>, to exploit optimizations for finding the points
to convolve.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># creating the filter + optimization
cConv = ContinuousConvBlock(input_numb_field=number_input_fileds,
                       output_numb_field=1,
                       filter_dim=filter_dim,
                       stride=stride,
                       optimize=True)
</pre></div>
</div>
<p>Let’s try to do a forward pass</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print(f&quot;Filter input data has shape: {data.shape}&quot;)

#input to the filter
output = cConv(data)

print(f&quot;Filter output data has shape: {output.shape}&quot;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Filter</span> <span class="nb">input</span> <span class="n">data</span> <span class="n">has</span> <span class="n">shape</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">Filter</span> <span class="n">output</span> <span class="n">data</span> <span class="n">has</span> <span class="n">shape</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">169</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
<p>If we don’t want to use the default <code class="docutils literal notranslate"><span class="pre">FeedForward</span></code> neural network, we
can pass a specified torch model in the <code class="docutils literal notranslate"><span class="pre">model</span></code> keyword as follow:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class SimpleKernel(torch.nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self. model = torch.nn.Sequential(
            torch.nn.Linear(2, 20),
            torch.nn.ReLU(),
            torch.nn.Linear(20, 20),
            torch.nn.ReLU(),
            torch.nn.Linear(20, 1))

    def forward(self, x):
        return self.model(x)


cConv = ContinuousConvBlock(input_numb_field=number_input_fileds,
                       output_numb_field=1,
                       filter_dim=filter_dim,
                       stride=stride,
                       optimize=True,
                       model=SimpleKernel)
</pre></div>
</div>
<p>Notice that we pass the class and not an already built object!</p>
</section>
</section>
<section id="building-a-mnist-classifier">
<h2>Building a MNIST Classifier<a class="headerlink" href="#building-a-mnist-classifier" title="Link to this heading">#</a></h2>
<p>Let’s see how we can build a MNIST classifier using a continuous
convolutional filter. We will use the MNIST dataset from PyTorch. In
order to keep small training times we use only 6000 samples for training
and 1000 samples for testing.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from torch.utils.data import DataLoader, SubsetRandomSampler

numb_training = 6000  # get just 6000 images for training
numb_testing= 1000  # get just 1000 images for training
seed = 111          # for reproducibility
batch_size = 8      # setting batch size

# setting the seed
torch.manual_seed(seed)

# downloading the dataset
train_data = torchvision.datasets.MNIST(&#39;./data/&#39;, train=True, download=True,
                                        transform=torchvision.transforms.Compose([
                                            torchvision.transforms.ToTensor(),
                                            torchvision.transforms.Normalize(
                                                (0.1307,), (0.3081,))
                                        ]))
subsample_train_indices = torch.randperm(len(train_data))[:numb_training]
train_loader = DataLoader(train_data, batch_size=batch_size,
                          sampler=SubsetRandomSampler(subsample_train_indices))

test_data = torchvision.datasets.MNIST(&#39;./data/&#39;, train=False, download=True,
                                        transform=torchvision.transforms.Compose([
                                            torchvision.transforms.ToTensor(),
                                            torchvision.transforms.Normalize(
                                                (0.1307,), (0.3081,))
                                        ]))
subsample_test_indices = torch.randperm(len(train_data))[:numb_testing]
test_loader = DataLoader(train_data, batch_size=batch_size,
                          sampler=SubsetRandomSampler(subsample_train_indices))
</pre></div>
</div>
<p>Let’s now build a simple classifier. The MNIST dataset is composed by
vectors of shape <code class="docutils literal notranslate"><span class="pre">[batch,</span> <span class="pre">1,</span> <span class="pre">28,</span> <span class="pre">28]</span></code>, but we can image them as one
field functions where the pixels <span class="math notranslate nohighlight">\(ij\)</span> are the coordinate
<span class="math notranslate nohighlight">\(x=i, y=j\)</span> in a <span class="math notranslate nohighlight">\([0, 27]\times[0,27]\)</span> domain, and the pixels
value are the field values. We just need a function to transform the
regular tensor in a tensor compatible for the continuous filter:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def transform_input(x):
    batch_size = x.shape[0]
    dim_grid = tuple(x.shape[:-3:-1])

    # creating the n dimensional mesh grid for a single channel image
    values_mesh = [torch.arange(0, dim).float() for dim in dim_grid]
    mesh = torch.meshgrid(values_mesh)
    coordinates_mesh = [x.reshape(-1, 1) for x in mesh]
    coordinates = torch.cat(coordinates_mesh, dim=1).unsqueeze(
        0).repeat((batch_size, 1, 1)).unsqueeze(1)

    return torch.cat((coordinates, x.flatten(2).unsqueeze(-1)), dim=-1)


# let&#39;s try it out
image, s = next(iter(train_loader))
print(f&quot;Original MNIST image shape: {image.shape}&quot;)

image_transformed = transform_input(image)
print(f&quot;Transformed MNIST image shape: {image_transformed.shape}&quot;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Original</span> <span class="n">MNIST</span> <span class="n">image</span> <span class="n">shape</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span>
<span class="n">Transformed</span> <span class="n">MNIST</span> <span class="n">image</span> <span class="n">shape</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
<p>We can now build a simple classifier! We will use just one convolutional
filter followed by a feedforward neural network</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># setting the seed
torch.manual_seed(seed)

class ContinuousClassifier(torch.nn.Module):
    def __init__(self):
        super().__init__()

        # number of classes for classification
        numb_class = 10

        # convolutional block
        self.convolution = ContinuousConvBlock(input_numb_field=1,
                                          output_numb_field=4,
                                          stride={&quot;domain&quot;: [27, 27],
                                                  &quot;start&quot;: [0, 0],
                                                  &quot;jumps&quot;: [4, 4],
                                                  &quot;direction&quot;: [1, 1.],
                                                  },
                                          filter_dim=[4, 4],
                                          optimize=True)
        # feedforward net
        self.nn = FeedForward(input_dimensions=196,
                              output_dimensions=numb_class,
                              layers=[120, 64],
                              func=torch.nn.ReLU)

    def forward(self, x):
        # transform input + convolution
        x = transform_input(x)
        x = self.convolution(x)
        # feed forward classification
        return self.nn(x[..., -1].flatten(1))


net = ContinuousClassifier()
</pre></div>
</div>
<p>Let’s try to train it using a simple pytorch training loop. We train for
juts 1 epoch using Adam optimizer with a <span class="math notranslate nohighlight">\(0.001\)</span> learning rate.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># setting the seed
torch.manual_seed(seed)

# optimizer and loss function
optimizer = torch.optim.Adam(net.parameters(), lr=0.001)
criterion = torch.nn.CrossEntropyLoss()

for epoch in range(1):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(train_loader, 0):
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 50 == 49:
            print(
                f&#39;batch [{i + 1}/{numb_training//batch_size}] loss[{running_loss / 500:.3f}]&#39;)
            running_loss = 0.0
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">batch</span> <span class="p">[</span><span class="mi">50</span><span class="o">/</span><span class="mi">750</span><span class="p">]</span> <span class="n">loss</span><span class="p">[</span><span class="mf">0.161</span><span class="p">]</span>
<span class="n">batch</span> <span class="p">[</span><span class="mi">100</span><span class="o">/</span><span class="mi">750</span><span class="p">]</span> <span class="n">loss</span><span class="p">[</span><span class="mf">0.073</span><span class="p">]</span>
<span class="n">batch</span> <span class="p">[</span><span class="mi">150</span><span class="o">/</span><span class="mi">750</span><span class="p">]</span> <span class="n">loss</span><span class="p">[</span><span class="mf">0.063</span><span class="p">]</span>
<span class="n">batch</span> <span class="p">[</span><span class="mi">200</span><span class="o">/</span><span class="mi">750</span><span class="p">]</span> <span class="n">loss</span><span class="p">[</span><span class="mf">0.051</span><span class="p">]</span>
<span class="n">batch</span> <span class="p">[</span><span class="mi">250</span><span class="o">/</span><span class="mi">750</span><span class="p">]</span> <span class="n">loss</span><span class="p">[</span><span class="mf">0.044</span><span class="p">]</span>
<span class="n">batch</span> <span class="p">[</span><span class="mi">300</span><span class="o">/</span><span class="mi">750</span><span class="p">]</span> <span class="n">loss</span><span class="p">[</span><span class="mf">0.050</span><span class="p">]</span>
<span class="n">batch</span> <span class="p">[</span><span class="mi">350</span><span class="o">/</span><span class="mi">750</span><span class="p">]</span> <span class="n">loss</span><span class="p">[</span><span class="mf">0.053</span><span class="p">]</span>
<span class="n">batch</span> <span class="p">[</span><span class="mi">400</span><span class="o">/</span><span class="mi">750</span><span class="p">]</span> <span class="n">loss</span><span class="p">[</span><span class="mf">0.049</span><span class="p">]</span>
<span class="n">batch</span> <span class="p">[</span><span class="mi">450</span><span class="o">/</span><span class="mi">750</span><span class="p">]</span> <span class="n">loss</span><span class="p">[</span><span class="mf">0.046</span><span class="p">]</span>
<span class="n">batch</span> <span class="p">[</span><span class="mi">500</span><span class="o">/</span><span class="mi">750</span><span class="p">]</span> <span class="n">loss</span><span class="p">[</span><span class="mf">0.034</span><span class="p">]</span>
<span class="n">batch</span> <span class="p">[</span><span class="mi">550</span><span class="o">/</span><span class="mi">750</span><span class="p">]</span> <span class="n">loss</span><span class="p">[</span><span class="mf">0.036</span><span class="p">]</span>
<span class="n">batch</span> <span class="p">[</span><span class="mi">600</span><span class="o">/</span><span class="mi">750</span><span class="p">]</span> <span class="n">loss</span><span class="p">[</span><span class="mf">0.040</span><span class="p">]</span>
<span class="n">batch</span> <span class="p">[</span><span class="mi">650</span><span class="o">/</span><span class="mi">750</span><span class="p">]</span> <span class="n">loss</span><span class="p">[</span><span class="mf">0.028</span><span class="p">]</span>
<span class="n">batch</span> <span class="p">[</span><span class="mi">700</span><span class="o">/</span><span class="mi">750</span><span class="p">]</span> <span class="n">loss</span><span class="p">[</span><span class="mf">0.040</span><span class="p">]</span>
<span class="n">batch</span> <span class="p">[</span><span class="mi">750</span><span class="o">/</span><span class="mi">750</span><span class="p">]</span> <span class="n">loss</span><span class="p">[</span><span class="mf">0.040</span><span class="p">]</span>
</pre></div>
</div>
<p>Let’s see the performance on the train set!</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>correct = 0
total = 0
with torch.no_grad():
    for data in test_loader:
        images, labels = data
        # calculate outputs by running images through the network
        outputs = net(images)
        # the class with the highest energy is what we choose as prediction
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(
    f&#39;Accuracy of the network on the 1000 test images: {(correct / total):.3%}&#39;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Accuracy</span> <span class="n">of</span> <span class="n">the</span> <span class="n">network</span> <span class="n">on</span> <span class="n">the</span> <span class="mi">1000</span> <span class="n">test</span> <span class="n">images</span><span class="p">:</span> <span class="mf">92.733</span><span class="o">%</span>
</pre></div>
</div>
<p>As we can see we have very good performance for having traing only for 1
epoch! Nevertheless, we are still using structured data… Let’s see how
we can build an autoencoder for unstructured data now.</p>
</section>
<section id="building-a-continuous-convolutional-autoencoder">
<h2>Building a Continuous Convolutional Autoencoder<a class="headerlink" href="#building-a-continuous-convolutional-autoencoder" title="Link to this heading">#</a></h2>
<p>Just as toy problem, we will now build an autoencoder for the following
function <span class="math notranslate nohighlight">\(f(x,y)=\sin(\pi x)\sin(\pi y)\)</span> on the unit circle domain
centered in <span class="math notranslate nohighlight">\((0.5, 0.5)\)</span>. We will also see the ability to
up-sample (once trained) the results without retraining. Let’s first
create the input and visualize it, we will use firstly a mesh of
<span class="math notranslate nohighlight">\(100\)</span> points.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># create inputs
def circle_grid(N=100):
    &quot;&quot;&quot;Generate points withing a unit 2D circle centered in (0.5, 0.5)

        :param N: number of points
        :type N: float
        :return: [x, y] array of points
        :rtype: torch.tensor
        &quot;&quot;&quot;

    PI = torch.acos(torch.zeros(1)).item() * 2
    R = 0.5
    centerX = 0.5
    centerY = 0.5

    r = R * torch.sqrt(torch.rand(N))
    theta = torch.rand(N) * 2 * PI

    x = centerX + r * torch.cos(theta)
    y = centerY + r * torch.sin(theta)

    return torch.stack([x, y]).T

# create the grid
grid = circle_grid(500)

# create input
input_data = torch.empty(size=(1, 1, grid.shape[0], 3))
input_data[0, 0, :, :-1] = grid
input_data[0, 0, :, -1] = torch.sin(pi * grid[:, 0]) * torch.sin(pi * grid[:, 1])

# visualize data
plt.title(&quot;Training sample with 500 points&quot;)
plt.scatter(grid[:, 0], grid[:, 1], c=input_data[0, 0, :, -1])
plt.colorbar()
plt.show()
</pre></div>
</div>
<img alt="../../../_images/tutorial_32_0.png" src="../../../_images/tutorial_32_0.png" />
<p>Let’s now build a simple autoencoder using the continuous convolutional
filter. The data is clearly unstructured and a simple convolutional
filter might not work without projecting or interpolating first. Let’s
first build and <code class="docutils literal notranslate"><span class="pre">Encoder</span></code> and <code class="docutils literal notranslate"><span class="pre">Decoder</span></code> class, and then a
<code class="docutils literal notranslate"><span class="pre">Autoencoder</span></code> class that contains both.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class Encoder(torch.nn.Module):
    def __init__(self, hidden_dimension):
        super().__init__()

        # convolutional block
        self.convolution = ContinuousConvBlock(input_numb_field=1,
                                          output_numb_field=2,
                                          stride={&quot;domain&quot;: [1, 1],
                                                  &quot;start&quot;: [0, 0],
                                                  &quot;jumps&quot;: [0.05, 0.05],
                                                  &quot;direction&quot;: [1, 1.],
                                                  },
                                          filter_dim=[0.15, 0.15],
                                          optimize=True)
        # feedforward net
        self.nn = FeedForward(input_dimensions=400,
                              output_dimensions=hidden_dimension,
                              layers=[240, 120])

    def forward(self, x):
        # convolution
        x = self.convolution(x)
        # feed forward pass
        return self.nn(x[..., -1])


class Decoder(torch.nn.Module):
    def __init__(self, hidden_dimension):
        super().__init__()

        # convolutional block
        self.convolution = ContinuousConvBlock(input_numb_field=2,
                                          output_numb_field=1,
                                          stride={&quot;domain&quot;: [1, 1],
                                                  &quot;start&quot;: [0, 0],
                                                  &quot;jumps&quot;: [0.05, 0.05],
                                                  &quot;direction&quot;: [1, 1.],
                                                  },
                                          filter_dim=[0.15, 0.15],
                                          optimize=True)
        # feedforward net
        self.nn = FeedForward(input_dimensions=hidden_dimension,
                              output_dimensions=400,
                              layers=[120, 240])

    def forward(self, weights, grid):
        # feed forward pass
        x = self.nn(weights)
        # transpose convolution
        return torch.sigmoid(self.convolution.transpose(x, grid))
</pre></div>
</div>
<p>Very good! Notice that in the <code class="docutils literal notranslate"><span class="pre">Decoder</span></code> class in the <code class="docutils literal notranslate"><span class="pre">forward</span></code> pass
we have used the <code class="docutils literal notranslate"><span class="pre">.transpose()</span></code> method of the
<code class="docutils literal notranslate"><span class="pre">ContinuousConvolution</span></code> class. This method accepts the <code class="docutils literal notranslate"><span class="pre">weights</span></code> for
upsampling and the <code class="docutils literal notranslate"><span class="pre">grid</span></code> on where to upsample. Let’s now build the
autoencoder! We set the hidden dimension in the <code class="docutils literal notranslate"><span class="pre">hidden_dimension</span></code>
variable. We apply the sigmoid on the output since the field value is
between <span class="math notranslate nohighlight">\([0, 1]\)</span>.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class Autoencoder(torch.nn.Module):
    def __init__(self, hidden_dimension=10):
        super().__init__()

        self.encoder = Encoder(hidden_dimension)
        self.decoder = Decoder(hidden_dimension)

    def forward(self, x):
        # saving grid for later upsampling
        grid = x.clone().detach()
        # encoder
        weights = self.encoder(x)
        # decoder
        out = self.decoder(weights, grid)
        return out

net = Autoencoder()
</pre></div>
</div>
<p>Let’s now train the autoencoder, minimizing the mean square error loss
and optimizing using Adam. We use the <code class="docutils literal notranslate"><span class="pre">SupervisedSolver</span></code> as solver,
and the problem is a simple problem created by inheriting from
<code class="docutils literal notranslate"><span class="pre">AbstractProblem</span></code>. It takes approximately two minutes to train on CPU.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># define the problem
class CircleProblem(AbstractProblem):
    input_variables = [&#39;x&#39;, &#39;y&#39;, &#39;f&#39;]
    output_variables = input_variables
    conditions = {&#39;data&#39; : Condition(input_points=LabelTensor(input_data, input_variables), output_points=LabelTensor(input_data, output_variables))}

# define the solver
solver = SupervisedSolver(problem=CircleProblem(), model=net, loss=torch.nn.MSELoss())

# train
trainer = Trainer(solver, max_epochs=150, accelerator=&#39;cpu&#39;, enable_model_summary=False) # we train on CPU and avoid model summary at beginning of training (optional)
trainer.train()
</pre></div>
</div>
<pre class="literal-block"><cite>Trainer.fit</cite> stopped: <cite>max_epochs=150</cite> reached.</pre>
<p>Let’s visualize the two solutions side by side!</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>net.eval()

# get output and detach from computational graph for plotting
output = net(input_data).detach()

# visualize data
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 3))
pic1 = axes[0].scatter(grid[:, 0], grid[:, 1], c=input_data[0, 0, :, -1])
axes[0].set_title(&quot;Real&quot;)
fig.colorbar(pic1)
plt.subplot(1, 2, 2)
pic2 = axes[1].scatter(grid[:, 0], grid[:, 1], c=output[0, 0, :, -1])
axes[1].set_title(&quot;Autoencoder&quot;)
fig.colorbar(pic2)
plt.tight_layout()
plt.show()
</pre></div>
</div>
<img alt="../../../_images/tutorial_40_0.png" src="../../../_images/tutorial_40_0.png" />
<p>As we can see the two are really similar! We can compute the <span class="math notranslate nohighlight">\(l_2\)</span>
error quite easily as well:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def l2_error(input_, target):
    return torch.linalg.norm(input_-target, ord=2)/torch.linalg.norm(input_, ord=2)


print(f&#39;l2 error: {l2_error(input_data[0, 0, :, -1], output[0, 0, :, -1]):.2%}&#39;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">l2</span> <span class="n">error</span><span class="p">:</span> <span class="mf">4.32</span><span class="o">%</span>
</pre></div>
</div>
<p>More or less <span class="math notranslate nohighlight">\(4\%\)</span> in <span class="math notranslate nohighlight">\(l_2\)</span> error, which is really low
considering the fact that we use just <strong>one</strong> convolutional layer and a
simple feedforward to decrease the dimension. Let’s see now some
peculiarity of the filter.</p>
<section id="filter-for-upsampling">
<h3>Filter for upsampling<a class="headerlink" href="#filter-for-upsampling" title="Link to this heading">#</a></h3>
<p>Suppose we have already the hidden dimension and we want to upsample on
a differen grid with more points. Let’s see how to do it:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># setting the seed
torch.manual_seed(seed)

grid2 = circle_grid(1500) # triple number of points
input_data2 = torch.zeros(size=(1, 1, grid2.shape[0], 3))
input_data2[0, 0, :, :-1] = grid2
input_data2[0, 0, :, -1] = torch.sin(pi *
                                    grid2[:, 0]) * torch.sin(pi * grid2[:, 1])

# get the hidden dimension representation from original input
latent = net.encoder(input_data)

# upsample on the second input_data2
output = net.decoder(latent, input_data2).detach()

# show the picture
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 3))
pic1 = axes[0].scatter(grid2[:, 0], grid2[:, 1], c=input_data2[0, 0, :, -1])
axes[0].set_title(&quot;Real&quot;)
fig.colorbar(pic1)
plt.subplot(1, 2, 2)
pic2 = axes[1].scatter(grid2[:, 0], grid2[:, 1], c=output[0, 0, :, -1])
axes[1].set_title(&quot;Up-sampling&quot;)
fig.colorbar(pic2)
plt.tight_layout()
plt.show()
</pre></div>
</div>
<img alt="../../../_images/tutorial_45_0.png" src="../../../_images/tutorial_45_0.png" />
<p>As we can see we have a very good approximation of the original
function, even thought some noise is present. Let’s calculate the error
now:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print(f&#39;l2 error: {l2_error(input_data2[0, 0, :, -1], output[0, 0, :, -1]):.2%}&#39;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">l2</span> <span class="n">error</span><span class="p">:</span> <span class="mf">8.49</span><span class="o">%</span>
</pre></div>
</div>
</section>
<section id="autoencoding-at-different-resolution">
<h3>Autoencoding at different resolution<a class="headerlink" href="#autoencoding-at-different-resolution" title="Link to this heading">#</a></h3>
<p>In the previous example we already had the hidden dimension (of original
input) and we used it to upsample. Sometimes however we have a more fine
mesh solution and we simply want to encode it. This can be done without
retraining! This procedure can be useful in case we have many points in
the mesh and just a smaller part of them are needed for training. Let’s
see the results of this:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># setting the seed
torch.manual_seed(seed)

grid2 = circle_grid(3500)  # very fine mesh
input_data2 = torch.zeros(size=(1, 1, grid2.shape[0], 3))
input_data2[0, 0, :, :-1] = grid2
input_data2[0, 0, :, -1] = torch.sin(pi *
                                     grid2[:, 0]) * torch.sin(pi * grid2[:, 1])

# get the hidden dimension representation from more fine mesh input
latent = net.encoder(input_data2)

# upsample on the second input_data2
output = net.decoder(latent, input_data2).detach()

# show the picture
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 3))
pic1 = axes[0].scatter(grid2[:, 0], grid2[:, 1], c=input_data2[0, 0, :, -1])
axes[0].set_title(&quot;Real&quot;)
fig.colorbar(pic1)
plt.subplot(1, 2, 2)
pic2 = axes[1].scatter(grid2[:, 0], grid2[:, 1], c=output[0, 0, :, -1])
axes[1].set_title(&quot;Autoencoder not re-trained&quot;)
fig.colorbar(pic2)
plt.tight_layout()
plt.show()

# calculate l2 error
print(
    f&#39;l2 error: {l2_error(input_data2[0, 0, :, -1], output[0, 0, :, -1]):.2%}&#39;)
</pre></div>
</div>
<img alt="../../../_images/tutorial_49_0.png" src="../../../_images/tutorial_49_0.png" />
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">l2</span> <span class="n">error</span><span class="p">:</span> <span class="mf">8.59</span><span class="o">%</span>
</pre></div>
</div>
</section>
</section>
<section id="whats-next">
<h2>What’s next?<a class="headerlink" href="#whats-next" title="Link to this heading">#</a></h2>
<p>We have shown the basic usage of a convolutional filter. There are
additional extensions possible:</p>
<ol class="arabic simple">
<li><p>Train using Physics Informed strategies</p></li>
<li><p>Use the filter to build an unstructured convolutional autoencoder for
reduced order modelling</p></li>
<li><p>Many more…</p></li>
</ol>
</section>
</section>


                </article>
              
              
              
              
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-filter-background">Continuous filter background</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-function">Input function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stride">Stride</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#filter-definition">Filter definition</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-mnist-classifier">Building a MNIST Classifier</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-continuous-convolutional-autoencoder">Building a Continuous Convolutional Autoencoder</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#filter-for-upsampling">Filter for upsampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#autoencoding-at-different-resolution">Autoencoding at different resolution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#whats-next">What’s next?</a></li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../../../_sources/_rst/tutorials/tutorial4/tutorial.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2021-2024, PINA Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.4.7.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>