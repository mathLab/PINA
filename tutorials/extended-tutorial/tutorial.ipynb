{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINA: A Python Software for Scientific Machine Learning\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mathLab/PINA/blob/UniTS-Lecture/tutorials/extended-tutorial/tutorial.ipynb)\n",
    "\n",
    "<p align=\"left\">\n",
    "    <img src=\"imgs/pina_logo.png\" alt=\"PINA Logo\" width=\"90\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "Welcome to PINA!\n",
    "\n",
    "PINA [1] is an open-source Python library providing an intuitive interface for solving differential equations using Physics Informed Neural Networks (PINNs), Neural Operators (NOs) and Reduced Order Models (ROMs). Based on PyTorch and PyTorchLightning, PINA offers a simple and intuitive way to formalize a specific (differential) problem and solve it using neural networks. The approximated solution of a differential equation can be implemented using PINA in a few lines of code thanks to the intuitive and user-friendly interface.\n",
    "\n",
    "In this workshop, we will guide you through the PINA package, following a step-by-step approach that gradually increases in complexity. Our goal is to demonstrate how PINA can be utilized to solve a wide range of differential problems, starting with basic techniques and progressing to more advanced methods.\n",
    "\n",
    "We will begin by exploring traditional methods like *Reduced-Order Models (ROM)*. From there, we’ll delve into *Physics-Informed Neural Networks (PINNs)* and their variants, which integrate physical laws into deep learning models to provide more accurate and efficient solutions. As we advance, we’ll introduce *Neural Operators*, a powerful tool for learning mappings between infinite-dimensional spaces, enabling the resolution of even more complex problems.\n",
    "\n",
    "Throughout the tutorial, we’ll showcase various combinations of these methods to illustrate how they can complement each other. At the core of PINA’s functionality, you’ll learn how to leverage deep learning for scientific machine learning (SciML) tasks, ultimately gaining the ability to solve intricate real-world problems across multiple domains.\n",
    "\n",
    "By the end of this tutorial, you will have hands-on experience with PINA and a deeper understanding of how to apply it to various differential models and SciML challenges.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The PINA Workflow \n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"imgs/pina_wokflow.png\" alt=\"PINA Workflow\" width=\"900\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the foundation: the PINA workflow. In PINA, solving a differential problem involves four key steps:\n",
    "\n",
    "1. **Problem Definition**: The first step is to clearly define the mathematical problem and the physical conditions it must satisfy. PINA provides several specialized classes for this purpose, including `SpatialProblem`, `TimeDependentProblem`, `ParametricProblem`, `InverseProblem`, and the abstract base class `AbstractProblem`. These classes help you structure your problem based on its unique characteristics, whether it involves space, time, parameters, or inverse modeling.\n",
    "\n",
    "2. **Domain Sampling**: Next, you need to prepare the model's input by discretizing the physical domain or importing data from numerical solvers. This step is crucial for defining the resolution and scope of your model. PINA provides essential tools like the `Conditions` class and the geometry module to facilitate domain sampling and ensure that the input data aligns with the problem's requirements.\n",
    "\n",
    "3. **Model and Solver Selection**: After setting up the problem and domain, the next step is to build the model. In PINA, models are constructed as PyTorch modules, allowing flexibility and scalability. You also need to choose a solver strategy to optimize the model. PINA implements a variety of solvers and models, which can be used as standalone components, inherited for customization, or created from scratch, offering a range of options depending on your specific needs.\n",
    "\n",
    "4. **Training**: The final step is optimizing the model using the chosen solver strategy. PINA leverages PyTorch Lightning for this process, providing a robust framework for training. The `Trainer` class, a wrapper around the PyTorch Lightning Trainer, enhances this process with advanced features, enabling more efficient and flexible training strategies.\n",
    "\n",
    "By following these four steps, PINA streamlines the process of solving differential problems, making it easier to apply deep learning techniques to scientific computing challenges.\n",
    "\n",
    "We will now begin by solving a simple problem for demonstration purposes. In this example, we will generate a regression dataset and use PINA to implement a Bayesian regressor. Specifically, we will employ Monte Carlo (MC) dropout as a method to approximate Bayesian inference, showcasing how PINA can be used to tackle regression tasks with uncertainty estimation.\n",
    "\n",
    "\n",
    "## A Simple Regression Problem in PINA\n",
    "\n",
    "Consider the problem [2] of approximating the following function with a Neural Net model $\\mathcal{M}_{\\theta}$:\n",
    "$$y = x^3 + \\epsilon, \\quad \\epsilon\\sim\\mathcal{N}(0,9),$$\n",
    "by having only a set of $20$ observations $\\{x_i, y_i\\}_{i=1}^{20}$, with  $x_i \\sim\\mathcal{U}[-4, 4]\\;\\;\\forall i\\in(1,\\dots,20)$.\n",
    "\n",
    "In PINA to construct this kind of problem we will need to create a class, `BayesianProblem`, inheriting from the `AbstractProblem` class since our problem depends only on input/output data given from an oracle. Later we will see that when learning an Ordinary/Partial Differential Equation *without oracle data* we will need to create a problem by inheriting from different classes. In general, in PINA there are various base classes the user can decide to inherit from, here are some examples (more on the official [documentation](https://mathlab.github.io/PINA/)):\n",
    "* ``AbstractProblem`` $\\rightarrow$ any PINA problem inherits from here, it is usually used when input output data are given\n",
    "* ``SpatialProblem`` $\\rightarrow$ a problem with spatial variable(s) as input for the model (we will see more of this!)\n",
    "* ``TimeDependentProblem`` $\\rightarrow$ a problem with temporal variable(s) as input for the model (see [this tutorial](https://mathlab.github.io/PINA/_rst/tutorials/tutorial3/tutorial.html) if interested)\n",
    "* ``ParametricProblem`` $\\rightarrow$ a problem with parametric variable(s) ``parameter_domain`` as input (see [this tutorial](https://mathlab.github.io/PINA/_rst/tutorials/tutorial1/tutorial.html) if interested)\n",
    "* ``InverseProblem`` $\\rightarrow$ an inverse problem to estimate models' parameters from data (see [this tutorial](https://mathlab.github.io/PINA/_rst/tutorials/tutorial7/tutorial.html) if interested)\n",
    "\n",
    "The classes `SpatialProblem`, `TimeDependentProblem`, `ParametricProblem` are usually used for learning Ordinary/Partial Differential Equation when we do not have accessible data, as we will see in the PINN section. \n",
    "\n",
    "Let's now create the data and the problem class using `AbstractProblem`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "  from google.colab import files\n",
    "  def getLocalFiles():\n",
    "      _files = files.upload()\n",
    "      if len(_files) >0:\n",
    "        for k,v in _files.items():\n",
    "          open(k,'wb').write(v)\n",
    "  getLocalFiles()\n",
    "  !pip install smithers pina-mathlab h5py\n",
    "\n",
    "from pina import Condition, LabelTensor                                         # importing data structures for problem constrains definition and tensor manipulation\n",
    "from pina.problem import AbstractProblem                                        # importing the spatial problem class\n",
    "from pina.geometry import CartesianDomain                                       # importing the geometry module to sample spatial variables easily\n",
    "import torch                                                                    # import torch for tensor operations\n",
    "\n",
    "\n",
    "# (a) Data generation and plot\n",
    "domain = CartesianDomain({'x' : [-4, 4]})                                       # create the geometry domain\n",
    "x = domain.sample(n=20, mode=\"random\")                                          # sampling 20 random points in [-4, 4]\n",
    "y = LabelTensor(x.pow(3) + 9*torch.randn_like(x), 'y')                          # obtain target for regression, in PINA the torch.Tensor is labelled\n",
    "\n",
    "# (b) PINA Problem formulation\n",
    "class BayesianProblem(AbstractProblem):                                         # here we start creating the problem\n",
    "\n",
    "    output_variables = ['y']                                                    # define the output variable name, in PINA the tensors can be queried with strings and indeces\n",
    "    input_variables = ['x']                                                     # define the input variable name, in PINA the tensors can be queried with strings and indeces\n",
    "    conditions = {'data': Condition(input_points=x, output_points=y)}           # define the conditions, these are the constraints the model will have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We highlight two very important features of PINA\n",
    "\n",
    "1. Additionally to torch `Tensor` structure, PINA uses the `LabelTensor` structure. This is particularly useful for stacking more tensors together and indexing them using strings\n",
    "2. The `Condition` object imposes the constraints that the model $\\mathcal{M}_{\\theta}$ will satisfy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA - on the use of LabelTensor\n",
    "\n",
    "label_tensor = LabelTensor(torch.rand(3, 4), ['a', 'b', 'c', 'd'])              # We define a 2D tensor, and we index with ['a', 'b', 'c', 'd'] its columns \n",
    "\n",
    "print(f'The Label Tensor object, a very short introduction... \\n')\n",
    "print(label_tensor,'\\n')\n",
    "print(f'Torch methods can be used, {label_tensor.shape=}')\n",
    "print(f'also {label_tensor.requires_grad=} \\n')\n",
    "print(f'But we have labels as well, e.g. {label_tensor.labels=}')\n",
    "print(f'And we can slice with labels: \\n {label_tensor[\"a\"]=}')\n",
    "print(f'Similarly to: \\n {label_tensor[:, 0]=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now solve the problem using a simple PyTorch model with Dropout, which we will implement from scratch. It’s worth noting that PINA provides a wide range of state-of-the-art (SOTA) architectures in the `pina.model` module, which you can explore in more detail [here](https://mathlab.github.io/PINA/_rst/_code.html#models). For the solver, we will use a straightforward supervised learning approach by importing the `SupervisedSolver` from `pina.solvers`. This solver is designed to handle typical regression tasks effectively, by minimizing the following loss:\n",
    "$$\n",
    "\\mathcal{L}_{\\rm{problem}} = \\frac{1}{N}\\sum_{i=1}^N\n",
    "\\mathcal{L}(y_i - \\mathcal{M}_{\\theta}(x_i))\n",
    "$$\n",
    "where $\\mathcal{L}$ is a specific loss function, default Mean Square Error:\n",
    "$$\n",
    "\\mathcal{L}(v) = \\| v \\|^2_2.\n",
    "$$\n",
    "Let's now build the model and solve the problem, we will use the same architecture as in [2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4999: 100%|██████████| 1/1 [00:00<00:00, 142.79it/s, v_num=11, mean_loss=103.0]\n"
     ]
    }
   ],
   "source": [
    "from pina.solvers import SupervisedSolver                                       # importing the solver for optimizing the model \n",
    "from pina.trainer import Trainer                                                # importing the trainer for training the model \n",
    "\n",
    "class BayesianModel(torch.nn.Module):                                           # creating the Neural Network Model, visit https://pytorch.org/tutorials/beginner/pytorch_with_examples.html for info\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "                            torch.nn.Linear(1, 100),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Dropout(0.5),\n",
    "                            torch.nn.Linear(100, 1)\n",
    "                            )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "problem = BayesianProblem()                                                     # setup problem and sample data\n",
    "model   = BayesianModel()                                                       # setup bayesian model\n",
    "solver  = SupervisedSolver(problem, model)                                      # setup solver\n",
    "trainer = Trainer(solver=solver, max_epochs=5000, accelerator='cpu')            # train for 5000 epochs only on cpu (try to change in gpu if your device has it!)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, the model has been trained! Let's now visualize the solutions. Since we used Dropout for training, we have learned a probabilistic bayesian model (see [3] for further details). Thus, every time we will evaluate the forward pass on the input points $x_i$ we will obtain a different results. We can then compute the mean $\\mu_\\theta$ and the standard deviation $\\sigma_\\theta$ to understand how accurate the model is and its associated uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt                                                 # importing modules for plotting\n",
    "\n",
    "x_test = LabelTensor(torch.linspace(-6, 6, 100).reshape(-1, 1), 'x')            # getting a grid of data in [-6, 6]\n",
    "y_test = torch.stack([solver(x_test) for _ in range(1000)], dim=0)              # compute the solution by sampling 1000 NN weights\n",
    "y_mean, y_std = y_test.mean(0).detach(), y_test.std(0).detach()                 # compute mean and variance of the solution\n",
    "\n",
    "\n",
    "x_test = x_test.flatten()                                                       # plotting the results\n",
    "y_mean = y_mean.flatten()\n",
    "y_std  = y_std.flatten()\n",
    "plt.plot(x_test, y_mean, label=r'$\\mu_{\\theta}$')\n",
    "plt.fill_between(x_test, y_mean-3*y_std, y_mean+3*y_std, alpha=0.3, label=r'3$\\sigma_{\\theta}$')\n",
    "plt.plot(x_test, x_test.pow(3), label='true')\n",
    "plt.scatter(x, y, label='train data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINA for Reduced Order Modeling\n",
    "In the last section we used PINA for supervised learning tasks, now we will do a step forward and use PINA for learning a Reduced Order Model. Specifically, our objective will be to learn the solution of a parametric differential equation using a few set of high fidelity data (from simulations), and generalize to new parameters. Our dataset will then be composed of solutions $u$ and parameters of the differential equations $\\mu$, and the model we will build is a map from $\\mathcal{M}_\\theta (\\mu) \\approx u$.\n",
    "\n",
    "In particular we are going to use the Proper Orthogonal Decomposition with Radial Basis Function Interpolation(POD-RBF)[4,5]. Here we basically perform a dimensional reduction using the POD approach, and approximating the parametric solution manifold (at the reduced space) using an interpolator (RBF).\n",
    "\n",
    "We exploit the [Smithers](www.github.com/mathLab/Smithers) library to collect the parametric snapshots. In particular, we use the `NavierStokesDataset` class that contains a set of parametric solutions of the Navier-Stokes equations in a 2D L-shape domain. The parameter is the inflow velocity.\n",
    "The dataset is composed by 500 snapshots of the velocity (along $x$, $y$, and the magnitude) and pressure fields, and the corresponding parameter values.\n",
    "\n",
    "To visually check the solutions, let's plot also the data points and the reference solution: this is the expected output of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smithers.dataset import NavierStokesDataset\n",
    "dataset = NavierStokesDataset()\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(14, 3))\n",
    "for ax, p, u in zip(axs, dataset.params[:4], dataset.snapshots['mag(v)'][:4]):\n",
    "    ax.tricontourf(dataset.triang, u, levels=16)\n",
    "    ax.set_title(f'$\\mu$ = {p[0]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *snapshots* - aka the numerical solutions computed for several parameters - and the corresponding parameters are the only data we need to train the model, in order to predict the solution for any new test parameter.\n",
    "To properly validate the accuracy, we initially split the 500 snapshots into the training dataset (90% of the original data) and the testing one (the reamining 10%). It must be said that, to plug the snapshots into PINA, we have to cast them to `LabelTensor` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = torch.tensor(dataset.snapshots['mag(v)']).float()                           # getting the velocity magnitude\n",
    "p = torch.tensor(dataset.params).float()                                        # getting the parameter\n",
    "\n",
    "p = LabelTensor(p, labels=['mu'])                                               # transforming in LabelTensors\n",
    "u = LabelTensor(u, labels=[f's{i}' for i in range(u.shape[1])])\n",
    "\n",
    "ratio_train_test = 0.9                                                          # split train and test\n",
    "n = u.shape\n",
    "n_train = int(u.shape[0] * ratio_train_test)\n",
    "n_test = u - n_train\n",
    "u_train, u_test = u[:n_train], u[n_train:]\n",
    "p_train, p_test = p[:n_train], p[n_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now procede to build the ROM. We will use the POD (`PODBlock`) and the RBF (`RBFBlock`) objects to build a model (`torch.nn.Module`). Notice that other approaches such as Autoencoder for dimensionality reduction, and Neural Networks for interpolation are possible, and can be combined with the `ReducedOrderModelSolver` (see [here](https://mathlab.github.io/PINA/_rst/solvers/rom.html) the documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pina.model.layers import PODBlock, RBFBlock                                # importing the POD and RBF layers\n",
    "\n",
    "class PODRBF(torch.nn.Module):\n",
    "    def __init__(self, pod_rank, rbf_kernel):\n",
    "        super().__init__()\n",
    "        self.pod = PODBlock(pod_rank)\n",
    "        self.rbf = RBFBlock(kernel=rbf_kernel)\n",
    "            \n",
    "    def forward(self, x):                                                       # the forward method find the coefficient of the POD expansion, and then expand the modes to reconstruct the field\n",
    "        coefficents = self.rbf(x)\n",
    "        return self.pod.expand(coefficents)\n",
    "\n",
    "    def fit(self, p, x):                                                        # the fit method is responsible to find POD reduction map, and the RBF interpolation map\n",
    "        self.pod.fit(x)\n",
    "        self.rbf.fit(p, self.pod.reduce(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now time to define the problem, fit the model and show the result! We inherit again from `AbstractProblem`, just defining a simple *input-output* condition. Notice that we will not use a trainer, since there are not parameters to be optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnapshotProblem(AbstractProblem):\n",
    "    output_variables = [f's{i}' for i in range(u.shape[1])]\n",
    "    parameter_domain = CartesianDomain({'mu': [0, 100]})\n",
    "    conditions = {\n",
    "        'io': Condition(input_points=p_train, output_points=u_train)\n",
    "    }\n",
    "\n",
    "poisson_problem = SnapshotProblem()                                             # create the problem\n",
    "pod_rbf = PODRBF(pod_rank=20, rbf_kernel='thin_plate_spline')                   # create the model and fit, try to change the pod_rank and see what happens!\n",
    "pod_rbf.fit(p_train, u_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very nice! We can now visualize the results,along with the training and test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib                                                               # for plotting\n",
    "\n",
    "u_test_rbf = pod_rbf(p_test)                                                    # field reconstruction for train data\n",
    "u_train_rbf = pod_rbf(p_train)                                                  # field reconstruction for test data\n",
    "\n",
    "relative_error_train = torch.norm(u_train_rbf - u_train)/torch.norm(u_train)\n",
    "relative_error_test = torch.norm(u_test_rbf - u_test)/torch.norm(u_test)\n",
    "\n",
    "print('Error summary for POD-RBF model:')                                       # printing the error\n",
    "print(f'  Train: {relative_error_train.item():e}')\n",
    "print(f'  Test:  {relative_error_test.item():e}')\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(14, 3))                                  # visualizing the results\n",
    "snapshot_number = 10\n",
    "cm = axs[0].tricontourf(dataset.triang,\n",
    "                   u_test[snapshot_number].flatten(), levels=16)\n",
    "axs[0].set_title(f'Real $\\mu$ = {float(p[snapshot_number]):.2f}')\n",
    "plt.colorbar(cm, ax=axs[0])\n",
    "cm = axs[1].tricontourf(dataset.triang,\n",
    "                   u_test_rbf[snapshot_number].flatten(),levels=16)\n",
    "axs[1].set_title(f'Reconstruction $\\mu$ = {float(p[snapshot_number]):.2f}')\n",
    "plt.colorbar(cm, ax=axs[1])\n",
    "cm = axs[2].tricontourf(dataset.triang,\n",
    "                   (u_test_rbf[snapshot_number] - u_test[snapshot_number]\n",
    "                    ).abs().flatten()\n",
    "                   , levels=16)\n",
    "axs[2].set_title(f'Absolute error $\\mu$ = {float(p[snapshot_number]):.2f}')\n",
    "plt.colorbar(cm, ax=axs[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINA for Physics Informed Machine Learning\n",
    "\n",
    "In the previous section we used PINA for supervised learning, but one of its true strengths lies in physics-informed machine learning. One of the most impactful approaches in this domain is through Physics-Informed Neural Networks (PINNs)[6]. PINNs are deep learning models that seamlessly integrate the laws of physics into the training process. By embedding physical principles—such as differential equations and boundary conditions—directly into the neural network's loss function, PINNs enable the modeling of complex physical systems while ensuring that the predictions remain consistent with established scientific laws. At the end, the PINN methodology can be used to approximate, independently of the discretization, the solution to any differential equation, i.e. $\\mathcal{M}_{\\theta} \\approx u$, with $u$ being the solution of a differential problem.\n",
    "\n",
    "In particular, we will see how to solve a 2D Poisson problem with Dirichlet boundary conditions, on a hourglass-shaped domain, using different variants of PINNs solvers, namely, standard PINNs [6], Self-Adaptive PINNs (*SAPINNs*) [7] and Residual-Based Attention PINNs (*RBAPINNs*) [8]. Other variants of PINNs are available on PINA, so if you are interested we definitely suggest you to look at PINA solvers on the [documentation](https://mathlab.github.io/PINA/_rst/_code.html#solvers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already done, we start by writing the problem class.\n",
    "We want to solve the following two-dimensional Poisson problem:\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\Delta u(x, y) = \\sin{(\\pi x)} \\sin{(\\pi y)} \\text{ in } D, \\\\\n",
    "u(x, y) = 0 \\text{ on } \\partial D \n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "where $D$ is a hourglass-shaped domain defined as the difference between a Cartesian domain and two ellipsoids intersecting it, and $\\partial D$ is the boundary of the hourglass.\n",
    "\n",
    "Before writing the problem, we will show you how to build complex domains. One of the greatest features implemented in PINA is the possibility to build custom geometries in a simple and efficient manner. PINA already offeres many built-in domain shapes and Boolean operators to combine them; in alternative, custom locations can be defined from scratch. If you are interested to investigate deeply the `geometry` module of PINA, look at [this Tutorial](https://mathlab.github.io/PINA/_rst/tutorials/tutorial6/tutorial.html).\n",
    "We will define the hourglass-shaped domain using the already existing classes `CartesianDomain` and `EllipsoidDomain`, exploiting the Boolean classes `Difference` and `Union`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pina.geometry import EllipsoidDomain, Difference, CartesianDomain, Union   # importing the geometry class\n",
    "\n",
    "# (a) Building the interior of the hourglass-shaped domain\n",
    "cartesian = CartesianDomain({'x': [-3, 3], 'y': [-3, 3]})                       # building a Cartesian domain\n",
    "ellipsoid_1 = EllipsoidDomain({'x': [-5, -1], 'y': [-3, 3]})                    # building an Ellipse\n",
    "ellipsoid_2 = EllipsoidDomain({'x': [1, 5], 'y': [-3, 3]})\n",
    "interior = Difference([cartesian,ellipsoid_1,ellipsoid_2])                      # removing from Cartesian domain the two ellipsis \n",
    "\n",
    "# (a) Building the boundary of the hourglass-shaped domain\n",
    "border_ellipsoid_1 = EllipsoidDomain({'x': [-5, -1], 'y': [-3, 3]},             # sampling the border of the EllipsoidDomain (sample_surface=True activate sampling only on the border)\n",
    "                                     sample_surface=True)\n",
    "border_ellipsoid_2 = EllipsoidDomain({'x': [1, 5], 'y': [-3, 3]},\n",
    "                                     sample_surface=True)\n",
    "border_1 = CartesianDomain({'x': [-3, 3], 'y': 3})\n",
    "border_2 = CartesianDomain({'x': [-3, 3], 'y': -3})\n",
    "ex_1 = CartesianDomain({'x': [-5, -3], 'y': [-3, 3]})\n",
    "ex_2 = CartesianDomain({'x': [3,5], 'y': [-3, 3]})\n",
    "border_ells = Union([border_ellipsoid_1,border_ellipsoid_2])\n",
    "border = Union([\n",
    "                border_1,\n",
    "                border_2,\n",
    "                Difference([\n",
    "                            Union([border_ellipsoid_1,border_ellipsoid_2]),\n",
    "                            ex_1,\n",
    "                            ex_2\n",
    "                            ])\n",
    "                ])\n",
    "\n",
    "# (c) Sample the domains\n",
    "interior_samples = interior.sample(n=1000, mode='random')                       # Once the domains are built we can sample inside the domains\n",
    "border_samples = border.sample(n=1000, mode='random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Now that we have built the domain, let's try to plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(interior_samples.extract('x'),\n",
    "            interior_samples.extract('y'),\n",
    "            c='blue', alpha=0.5)\n",
    "plt.title(\"Hourglass Interior\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(border_samples.extract('x'),\n",
    "            border_samples.extract('y'),\n",
    "            c='blue', alpha=0.5)\n",
    "plt.title(\"Hourglass Border\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very good! We will know implement the problem class. Differently from the previous examples in which we used `AbstractProblem` as super class, to build the Poisson problem we will inherit from `SpatialProblem`.\n",
    "This is because we will build a model whose inputs are spatial variable(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pina.problem import SpatialProblem                                         # importing the SpatialProblem class\n",
    "from pina.operators import laplacian                                            # importing PINA differential operators, already available \n",
    "from pina.equation import FixedValue, Equation                                  # importing Dirichlet conditions and a standard Equation wrapper\n",
    "\n",
    "def poisson_equation(input_, output_):                                          # here we build a simple function that returns the residual od the poisson equation, i.e. residual = Δu - sin(πx)sin(πy)\n",
    "    force_term = (torch.sin(input_.extract(['x'])*torch.pi) *\n",
    "                    torch.sin(input_.extract(['y'])*torch.pi))\n",
    "    laplacian_u = laplacian(output_, input_, components=['u'], d=['x', 'y'])\n",
    "    return laplacian_u - force_term\n",
    "\n",
    "class Poisson(SpatialProblem):                                                  \n",
    "    output_variables = ['u']\n",
    "    spatial_domain = CartesianDomain({'x': [0, 1], 'y': [0, 1]})    \n",
    "    conditions = {\n",
    "        'border':   Condition(location=border,                                  # notice here we pass location and equation at the conditions. This means that we want the network to respect the equation at those locations.\n",
    "                              equation=FixedValue(0.)),\n",
    "        'interior': Condition(location=interior,\n",
    "                              equation=Equation(poisson_equation)),\n",
    "    }\n",
    "\n",
    "poisson_problem = Poisson()                                                     # creating the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, writing the problem class for a differential equation is still very easy! The main difference is the inheritance, and the fact that we use `location` and `equation` inside `Condition`. We will not go deeply inside the `Equation` class, but this object can be very useful for creating modular problem classes (see [this Tutorial](https://mathlab.github.io/PINA/_rst/tutorials/tutorial12/tutorial.html) if interested).\n",
    "\n",
    "Once the problem is set, we need to sample the domain to obtain the data. Don't worry, if you forget to sample an error will be raised before training 😉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Points are not automatically sampled, you can see this by:')\n",
    "print(f'    {poisson_problem.have_sampled_points=}\\n')\n",
    "print('But you can easily sample by running .discretise_domain:')\n",
    "poisson_problem.discretise_domain(n=1000, locations=['interior'])               # here we sample 1000 points in the interior\n",
    "poisson_problem.discretise_domain(n=100, locations=['border'])                  # here we sample 1000 points on the border\n",
    "print(f'    {poisson_problem.have_sampled_points=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have set the problem, and sampled the domain we need to build a model $\\mathcal{M}_{\\theta}$. For this, we will now use the custom PINA models available [here](https://mathlab.github.io/PINA/_rst/_code.html#models).\n",
    "We will use a feed-forward Neural Network by importing the class `FeedForward`. This Neural Network takes as input the coordinates (in this case $x$ and $y$) and provides the unkwown field of the Poisson problem. In this tutorial, the neural network is composed by two hidden layers of $10$ neurons each with `Tanh` activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pina.model import FeedForward                                              # importing the FeedForward model\n",
    "\n",
    "\n",
    "model = FeedForward(\n",
    "    func = torch.nn.Tanh,\n",
    "    layers=[10]*2,\n",
    "    output_dimensions=len(poisson_problem.output_variables),                    # the output dimension is the field dimension\n",
    "    input_dimensions=len(poisson_problem.input_variables)                       # the input dimension is the coordinates dimension\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last part of the PINA pipeline requires a `Solver` and a `Trainer`. As we said before, we will use the classical `PINN` solver and its variants, namely `SAPINN` and `RBAPINN`. We brifely report below how their loss function is defined:\n",
    "\n",
    "**Classical PINN**\n",
    "$$\\theta_{\\rm{best}}=\\min_{\\theta}\\mathcal{L}_{\\rm{problem}}(\\theta), \\quad  \\mathcal{L}_{\\rm{problem}}(\\theta)= \\frac{1}{N_{D}}\\sum_{i=1}^N\n",
    "\\mathcal{L}(\\Delta\\mathcal{M}_{\\theta}(\\mathbf{x}_i, \\mathbf{y}_i) - \\sin(\\pi x_i)\\sin(\\pi y_i)) +\n",
    "\\frac{1}{N}\\sum_{i=1}^N\n",
    "\\mathcal{L}(\\mathcal{M}_{\\theta}(\\mathbf{x}_i, \\mathbf{y}_i))$$\n",
    "\n",
    "**SAPINN**\n",
    "$$\\theta_{\\rm{best}}=\\min_{\\theta}\\max_{\\lambda_{\\Omega}, \\lambda_{\\partial \\Omega}}\\mathcal{L}_{\\rm{problem}}(\\theta), \\quad  \\mathcal{L}_{\\rm{problem}}(\\theta)= \\frac{1}{N_{D}}\\sum_{i=1}^N\n",
    "m(\\lambda_{\\Omega}^i)\\mathcal{L}(\\Delta\\mathcal{M}_{\\theta}(\\mathbf{x}_i, \\mathbf{y}_i) - \\sin(\\pi x_i)\\sin(\\pi y_i)) +\n",
    "\\frac{1}{N}\\sum_{i=1}^N\n",
    "m(\\lambda_{\\partial \\Omega}^i)\\mathcal{L}(\\mathcal{M}_{\\theta}(\\mathbf{x}_i, \\mathbf{y}_i))$$\n",
    "\n",
    "**RBAPINN**\n",
    "$$\\theta_{\\rm{best}}=\\min_{\\theta}\\mathcal{L}_{\\rm{problem}}(\\theta), \\quad  \\mathcal{L}_{\\rm{problem}}(\\theta)= \\frac{1}{N_{D}}\\sum_{i=1}^N\n",
    "\\lambda_{\\Omega}^i\\mathcal{L}(\\Delta\\mathcal{M}_{\\theta}(\\mathbf{x}_i, \\mathbf{y}_i) - \\sin(\\pi x_i)\\sin(\\pi y_i)) +\n",
    "\\frac{1}{N}\\sum_{i=1}^N\n",
    "\\lambda_{\\partial \\Omega}^i\\mathcal{L}(\\mathcal{M}_{\\theta}(\\mathbf{x}_i, \\mathbf{y}_i)), \\quad \\lambda^i_{k+1} \\leftarrow \\gamma\\lambda^i_{k} + \n",
    "        \\eta\\frac{\\lvert r_i\\rvert}{\\max_j \\lvert r_j\\rvert}\\;\\forall k,i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train for $1000$ epochs with default optimizers parameters. These parameters can be modified as desired, have a look at the solvers documentation [here](https://mathlab.github.io/PINA/_rst/_code.html#solvers). We use the `MetricTracker` class to track the metrics during training. We will not go into details of the many features the `Trainer` class has, but in [this Tutorial](https://mathlab.github.io/PINA/_rst/tutorials/tutorial11/tutorial.html) we cover the most important ones to boost model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pina.solvers import PINN, SAPINN, RBAPINN                                  # importing the solvers\n",
    "from pina.callbacks import MetricTracker                                        # importing the MetricTracker to track the loss during training\n",
    "\n",
    "\n",
    "trainers = {}                                                                   # create a dictionary of trainers, it will be used later for plotting\n",
    "for Solver in [PINN, SAPINN, RBAPINN]:                                          # iterate over solvers\n",
    "    print('Training with {Solver.__name__} solver')\n",
    "    solver = Solver(poisson_problem, model)\n",
    "    trainer = Trainer(solver,                                                   # we train on CPU for 1000 epochs and avoid model summary at beginning of training (optional)\n",
    "                      max_epochs=2000,\n",
    "                      callbacks=[MetricTracker()],\n",
    "                      accelerator='cpu',\n",
    "                      enable_model_summary=False)\n",
    "    trainer.train()\n",
    "    trainers[Solver.__name__] = trainer                                         # saving trainer\n",
    "    model.apply(                                                                # reset the model parameters\n",
    "        lambda layer: layer.reset_parameters()\n",
    "        if hasattr(layer, 'reset_parameters') else None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very nice! Let's see how the mean losses changed during training. For this, we will use the `Plotter` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pina import Plotter                                                        # importing the plotter class\n",
    "\n",
    "plotter = Plotter()\n",
    "for solver_name, trainer in trainers.items():                                   # iterate over the saved trainers\n",
    "    plotter.plot_loss(trainer, logy=True, label=solver_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the `RBAPINN` solver is the one better performing overall, i.e. achieving the lowest mean square error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINA for Autoregressive Neural Operators\n",
    "\n",
    "In the previous section, we explored the application of PINA in supervised learning and PINNs, but another fundamental advancement in scientific machine learning is through neural operators. Neural operators (NO) extend the capabilities of traditional neural networks by learning mappings between function spaces, i.e. operators, which makes them particularly suited for complex physical systems governed by partial differential equations (PDEs). For example, a NO could map the field at the initial temporal condition of a PDE, to the evolution at a specific time step; or the parameter of a differential equation to its solution for the specific parameter. Instead of operating on discrete inputs, neural operators approximate solutions in a continuous manner, making them independent of specific discretizations.\n",
    "\n",
    "In particular in this tutorial we will focus on Autoregressive Neural Operator [11]. We will focus on PDEs in one time dimension $t\\in[0,T]$, and (possibly) multiple spatial dimensions $\\mathbf{x}\\in\\Omega\\subset\\mathbb{R}^d$. We hone in on solving PDEs of the form:\n",
    "$$\n",
    "\\begin{split}\n",
    "    &\\partial_t u(\\mathbf{x},t) = F(\\mathbf{x}, t, u,\\partial_{\\mathbf{x}}u,\\dots) \\quad\\quad\\quad (\\mathbf{x},t)\\in\\Omega\\times[0,T]\\\\\n",
    "    &u(\\mathbf{x}, t=0)=u^0(\\mathbf{x}), \\quad B[u](t, \\mathbf{x}), \\;\\;\\,\\quad \\mathbf{x}\\in\\Omega, \\;(\\mathbf{x},t)\\in\\partial\\Omega\\times[0,T] \n",
    "\\end{split}\n",
    "$$\n",
    "where $u:\\Omega\\times[0,T] \\rightarrow \\mathbb{R}^n$ is the solution, with $u^0$ initial condition at $t=0$ and boundary condition $B[u]$ (e.g. Dirichlet) on the boundary $\\partial\\Omega$ of the domain. The Autoregressive Neural Operator is a model $\\mathcal{M}_\\theta$ that maps the solution at time $t$ to the one at time $t+\\delta$:\n",
    "$$\n",
    "u_{t+\\delta} \\approx \\mathcal{M}_\\theta(u_t), \\quad \\delta>0.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus on solving the **Kuramoto-Sivashinsky** equation:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{cases}\n",
    "\\partial_t u + \\partial_{xxxx} u + \\partial_{xx} u + u \\partial_x u = 0 \\quad\\quad\\quad x \\in [-\\frac{L}{2},\\frac{L}{2}], \\quad t \\in [0,T] \\\\\n",
    "u(-\\frac{L}{2},t) = u (\\frac{L}{2},t) \\quad\\quad\\quad t \\in [0,T]\\\\\n",
    "u(x,0) = f(x,0)\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "where $f$ is a sum of long-wavelength sinusoidal function. \n",
    "\n",
    "Training and test data have been generated exploiting the methodology described in [12]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import Dataset\n",
    "\n",
    "# get the data\n",
    "if IN_COLAB:\n",
    "    !mkdir \"data\"\n",
    "    !wget \"https://github.com/mathLab/PINA/raw/refs/heads/MDS2024/tutorials/extended-tutorial/data/KS_train.h5\" -O \"data/KS_train.h5\"\n",
    "    !wget \"https://github.com/mathLab/PINA/raw/refs/heads/MDS2024/tutorials/extended-tutorial/data/KS_test.h5\" -O \"data/KS_test.h5\"\n",
    "# get the data\n",
    "data_train = Dataset('data/KS_train.h5')\n",
    "data_test = Dataset('data/KS_test.h5')\n",
    "\n",
    "# define the problem\n",
    "class NOProblem(AbstractProblem):\n",
    "    input_variables = ['u']\n",
    "    output_variables = ['u']\n",
    "    conditions = {'data' : Condition(input_points=data_train.pde,\n",
    "                                    output_points=data_train.pde)}\n",
    "no_problem = NOProblem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to construct the Autoregressive Solver, following [11]. \n",
    "We need to define 1) the loss function used to train the solver 2) the unrolling procedure for predicting over the test data:\n",
    "\n",
    "1) for each batch, at each epoch, a random interval $[t,t+\\delta]$ is selected (in our solver, $\\delta=1$) and the difference $du_{\\delta}(x,t) = u(x,t+\\delta)-u(x,t)$ is computed; the neural network computes an approximation of $du_{\\delta}$ taking $u$ in input, i.e. $\\overline{du}_{\\delta}(x,t) = \\mathcal{M}_{\\theta} (u(x,t))$; eventually, the MSE loss between $du$ and $\\overline{du}_{\\delta}(x,t)$ is computed;\n",
    "2) during inference phase, $\\overline{du}_{\\delta}(x,t)$ is computed for each timestep $t$ in the specified unrolling interval. In our data, the timestep discretization is equal to $dt=0.1$ sec.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoregressiveSolver(SupervisedSolver):\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def unroll(self, input, unrollings=100):\n",
    "        \"\"\"\n",
    "        Unrolling the prediction from the initial input for unrollings steps.\n",
    "        \"\"\"\n",
    "        trajectory = [input]\n",
    "        for _ in range(unrollings):\n",
    "            du_hat_t = self.neural_net.torchmodel(trajectory[-1])\n",
    "            trajectory.append(trajectory[-1] + du_hat_t)\n",
    "        return torch.stack(trajectory, 1)\n",
    "    \n",
    "    def loss_data(self, input_pts, output_pts):\n",
    "        \"\"\"\n",
    "        Computing the variational loss for an autoregressive solver. The \n",
    "        output have shape [B, Nt, Nx, D], input_pts are not used but kept for\n",
    "        consistency.\n",
    "        \"\"\"\n",
    "        # Extract a time interval\n",
    "        t_start = torch.randint(0,                                              # sampling random starting time\n",
    "                                output_pts.shape[1]-1,\n",
    "                                (output_pts.shape[0],),\n",
    "                                device=input_pts.device)\n",
    "        t_end = t_start + 1 \n",
    "        idx_batch = torch.arange(output_pts.shape[0])\n",
    "        u_start = output_pts[idx_batch, t_start, ...]\n",
    "        u_final = output_pts[idx_batch, t_end, ...]\n",
    "        du = u_final - u_start\n",
    "        # Forward pass\n",
    "        du_hat = self.forward(u_start)\n",
    "        # Compute loss\n",
    "        mse_loss = (du_hat - du).pow(2).mean().as_subclass(torch.Tensor)\n",
    "        return mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NO we use here is the **Fourier Neural Operator** (FNO) [9], already implemented in PINA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pina.model import FNO\n",
    "\n",
    "lifting_net = torch.nn.Linear(1, 64)\n",
    "projecting_net = torch.nn.Linear(64, 1)\n",
    "model = FNO(lifting_net=lifting_net,\n",
    "            projecting_net=projecting_net,\n",
    "            n_modes=32,\n",
    "            n_layers=5,\n",
    "            dimensions=1,\n",
    "            inner_size=64,\n",
    "            func=torch.nn.SiLU,\n",
    "            padding=8)\n",
    "\n",
    "trainer = Trainer(solver=AutoregressiveSolver(no_problem, model=model), max_epochs=200, batch_size=64, accelerator='cpu')\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training phase, we obtain the prediction to compare with the test data unrolling the model starting from the initial data `data_test.pde[:, 0, ...]` all over the time interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = trainer.model.unroll(input=data_test.pde[:, 0, ...],unrollings=int(data_test.pde.shape[1]-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the predictions and compare them with the test data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "idx = 1\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 3)) \n",
    "axs[0] = plt.subplot(1, 3, 1,)\n",
    "axs[0].imshow(data_test.pde[idx].squeeze(-1), origin='lower')\n",
    "axs[0].set_title('Reference solution')\n",
    "axs[1] = plt.subplot(1, 3, 2)\n",
    "axs[1].imshow(sol[idx].squeeze(-1), origin='lower')\n",
    "axs[1].set_title('Predicted solution')\n",
    "axs[2] = plt.subplot(1, 3, 3)\n",
    "diff = (data_test.pde[idx]-sol[idx]).abs().squeeze(-1)\n",
    "cm = axs[2].imshow(diff, origin='lower', norm=colors.LogNorm(vmin=diff.min().item()+1e-8, vmax=diff.max().item()),cmap='viridis')\n",
    "axs[2].set_title('Absolute error')\n",
    "cax = fig.add_axes([0.9, 0.11, 0.01, 0.77])\n",
    "fig.colorbar(cm, cax = cax, ax=axs[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are not bad, taking in account the complex nature of the Kuramoto-Sivashinsky equation, which usually requires hours of training to get accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GitHub Repo\n",
    "\n",
    "You can download this tutorial scanning *the QR code below*:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"imgs/qr-code.png\" alt=\"QR CODE\" width=300\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] *Coscia, Dario, et al. \"Physics-informed neural networks for advanced modeling.\" Journal of Open Source Software, 2023.*\n",
    "\n",
    "[2] *Hernández-Lobato, José Miguel, and Ryan Adams. \"Probabilistic backpropagation for scalable learning of bayesian neural networks.\" International conference on machine learning, 2015.*\n",
    "\n",
    "[3] *Gal, Yarin, and Zoubin Ghahramani. \"Dropout as a bayesian approximation: Representing model uncertainty in deep learning.\" International conference on machine learning, 2016.*\n",
    "\n",
    "[4] *Rozza, Gianluigi, Giovanni Stabile, and Francesco Ballarin, eds. Advanced reduced order methods and applications in computational fluid dynamics. Society for Industrial and Applied Mathematics, 2022.*\n",
    "\n",
    "[5] *Hesthaven, Jan S., and Stefano Ubbiali. \"Non-intrusive reduced order modeling of nonlinear problems using neural networks.\" Journal of Computational Physics, 2018*\n",
    "\n",
    "[6] *Raissi, Maziar, Paris Perdikaris, and George E. Karniadakis. \"Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.\" Journal of Computational Physics, 2019.*\n",
    "\n",
    "[7] *McClenny, Levi D., and Ulisses M. Braga-Neto. \"Self-adaptive physics-informed neural networks.\" Journal of Computational Physics, 2023.*\n",
    "\n",
    "[8] *Anagnostopoulos, Sokratis J., et al. \"Residual-based attention in physics-informed neural networks.\" Computer Methods in Applied Mechanics and Engineering, 2024.*\n",
    "\n",
    "[9] *Li, Zongyi, et al. \"Fourier Neural Operator for Parametric Partial Differential Equations.\" International Conference on Learning Representations, 2021.*\n",
    "\n",
    "[10] *Brandstetter, J., Worrall, D. E., & Welling, M. Message Passing Neural PDE Solvers. In International Conference on Learning Representations.*\n",
    "\n",
    "[11] *McCabe, M., Harrington, P., Subramanian, S., & Brown, J. Towards Stability of Autoregressive Neural Operators. Transactions on Machine Learning Research.* \n",
    "\n",
    "[12] *Bar-Sinai, Y., Hoyer, S., Hickey, J., & Brenner, M. P. (2019). Learning data-driven discretizations for partial differential equations. Proceedings of the National Academy of Sciences, 116(31), 15344-15349.*\n",
    "\n",
    "### Licese and Acknowledgement\n",
    "\n",
    "This tutorial is done with the help of the PINA contributors, which we acknowledge for their constant help in the develop and maintenance process of the software. We distribute this tutorial under the MIT License available [here](https://github.com/mathLab/PINA/blob/master/LICENSE.rst)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pina",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
