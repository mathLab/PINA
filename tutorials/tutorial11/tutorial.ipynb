{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: PINA and Pytorch Lightning, training tips and visualizations \n",
    "\n",
    "\n",
    "In this tutorial, we will delve deeper into the functionality of the `Trainer` class, which serves as the cornerstone for training **PINA** [Solvers](https://mathlab.github.io/PINA/_rst/_code.html#solvers). \n",
    "\n",
    "The `Trainer` class offers a plethora of features aimed at improving model accuracy, reducing training time and memory usage, facilitating logging visualization, and more.\n",
    "\n",
    "Our leading example will revolve around solving the `SimpleODE` problem, as outlined in the [*Introduction to PINA for Physics Informed Neural Networks training*](https://github.com/mathLab/PINA/blob/master/tutorials/tutorial1/tutorial.ipynb). If you haven't already explored it, we highly recommend doing so before diving into this tutorial.\n",
    "\n",
    "Let's start by importing useful modules, define the `SimpleODE` problem and the `PINN` solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from pina import Condition, Trainer\n",
    "from pina.solvers import PINN\n",
    "from pina.model import FeedForward\n",
    "from pina.problem import SpatialProblem\n",
    "from pina.operators import grad\n",
    "from pina.geometry import CartesianDomain\n",
    "from pina.equation import Equation, FixedValue\n",
    "\n",
    "class SimpleODE(SpatialProblem):\n",
    "\n",
    "    output_variables = ['u']\n",
    "    spatial_domain = CartesianDomain({'x': [0, 1]})\n",
    "\n",
    "    # defining the ode equation\n",
    "    def ode_equation(input_, output_):\n",
    "        u_x = grad(output_, input_, components=['u'], d=['x'])\n",
    "        u = output_.extract(['u'])\n",
    "        return u_x - u\n",
    "\n",
    "    # conditions to hold\n",
    "    conditions = {\n",
    "        'x0': Condition(location=CartesianDomain({'x': 0.}), equation=FixedValue(1)),             # We fix initial condition to value 1\n",
    "        'D': Condition(location=CartesianDomain({'x': [0, 1]}), equation=Equation(ode_equation)), # We wrap the python equation using Equation\n",
    "    }\n",
    "\n",
    "    # defining the true solution\n",
    "    def truth_solution(self, pts):\n",
    "        return torch.exp(pts.extract(['x']))\n",
    "    \n",
    "\n",
    "# sampling for training\n",
    "problem = SimpleODE()\n",
    "problem.discretise_domain(1, 'random', locations=['x0'])\n",
    "problem.discretise_domain(20, 'lh', locations=['D'])\n",
    "\n",
    "# build the model\n",
    "model = FeedForward(\n",
    "    layers=[10, 10],\n",
    "    func=torch.nn.Tanh,\n",
    "    output_dimensions=len(problem.output_variables),\n",
    "    input_dimensions=len(problem.input_variables)\n",
    ")\n",
    "\n",
    "# create the PINN object\n",
    "pinn = PINN(problem, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Till now we just followed the extact step of the previous tutorials. The `Trainer` object\n",
    "can be initialized by simiply passing the `PINN` solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(solver=pinn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Accelerator\n",
    "\n",
    "When creating the trainer, **by defualt** the `Trainer` will choose the most performing `accelerator` for training which is available in your system, ranked as follow:\n",
    "1. [TPU](https://cloud.google.com/tpu/docs/intro-to-tpu)\n",
    "2. [IPU](https://www.graphcore.ai/products/ipu)\n",
    "3. [HPU](https://habana.ai/)\n",
    "4. [GPU](https://www.intel.com/content/www/us/en/products/docs/processors/what-is-a-gpu.html#:~:text=What%20does%20GPU%20stand%20for,video%20editing%2C%20and%20gaming%20applications) or [MPS](https://developer.apple.com/metal/pytorch/)\n",
    "5. CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For setting manually the `accelerator` run:\n",
    "\n",
    "* `accelerator = {'gpu', 'cpu', 'hpu', 'mps', 'cpu', 'ipu'}` sets the accelerator to a specific one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(solver=pinn, accelerator='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you can see, even if in the used system `GPU` is available, it is not used since we set `accelerator='cpu'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Logging\n",
    "\n",
    "In `PINA` you can logged the metrics in different ways. The simplest one is to use the `MetricTraker` class from `pina.callbacks` as seen in the [*Introduction to PINA for Physics Informed Neural Networks training*](https://github.com/mathLab/PINA/blob/master/tutorials/tutorial1/tutorial.ipynb) tutorial.\n",
    "\n",
    "However, expecially when we need to train multiple times to get an average of the loss across multiple runs, `pytorch_lightning.loggers` might be useful. Here we will use `TensorBoardLogger` (more on [logging](https://lightning.ai/docs/pytorch/stable/extensions/logging.html) here), but you can choose the one you prefer (or make your own one) thanks to the amazing job done by the PytorchLightning team!\n",
    "\n",
    "We will now import `TensorBoardLogger`, do three runs of training and then visualize the results. Notice we set `enable_model_summary=False` to avoid model summary specifications (e.g. number of parameters), set it to true if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|██████████| 1/1 [00:00<00:00, 303.14it/s, v_num=3, x0_loss=1.09e-5, D_loss=0.000242, mean_loss=0.000127]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|██████████| 1/1 [00:00<00:00, 171.98it/s, v_num=3, x0_loss=1.09e-5, D_loss=0.000242, mean_loss=0.000127]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|██████████| 1/1 [00:00<00:00, 269.11it/s, v_num=4, x0_loss=1.97e-5, D_loss=0.000483, mean_loss=0.000251]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|██████████| 1/1 [00:00<00:00, 156.86it/s, v_num=4, x0_loss=1.97e-5, D_loss=0.000483, mean_loss=0.000251]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|██████████| 1/1 [00:00<00:00, 268.38it/s, v_num=5, x0_loss=2.44e-5, D_loss=0.000865, mean_loss=0.000444]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|██████████| 1/1 [00:00<00:00, 150.15it/s, v_num=5, x0_loss=2.44e-5, D_loss=0.000865, mean_loss=0.000444]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# three run of training, by default it trains for 1000 epochs\n",
    "# we reinitialize the model each time otherwise the same parameters will be optimized\n",
    "for _ in range(3):\n",
    "    model = FeedForward(\n",
    "        layers=[10, 10],\n",
    "        func=torch.nn.Tanh,\n",
    "        output_dimensions=len(problem.output_variables),\n",
    "        input_dimensions=len(problem.input_variables)\n",
    "    )\n",
    "    pinn = PINN(problem, model)\n",
    "    trainer = Trainer(solver=pinn, accelerator='cpu', logger=TensorBoardLogger(save_dir='simpleode'), enable_model_summary=False)\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize the logs by simply running `tensorboard --logdir=simpleode/` on terminal, you should obtain a webpage as the one shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\\\"center\\\">\n",
    "<img src=\"logging.png\" alt=\\\"Logging API\\\" width=\\\"400\\\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you can see, by default, **PINA** logs the losses which are shown in the progress bar, as well as the number of epochs. You can always insert more loggings by either defining a **callback** ([more on callbacks](https://lightning.ai/docs/pytorch/stable/extensions/callbacks.html)), or inheriting the solver and modify the programs with different **hooks** ([more on hooks](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html#hooks))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we need to access certain steps of the training for logging, do static modifications (i.e. not changing the solver) or updating problem hyperparameters (static variables), we can use `Callabacks`. Notice that `Callbacks` allow you to add arbitrary self-contained programs to your training. At specific points during the flow of execution (hooks), the Callback interface allows you to design programs that encapsulate a full set of functionality. It de-couples functionality that does not need to be in **PINA** `Solver`s.\n",
    "Lightning has a callback system to execute them when needed. Callbacks should capture NON-ESSENTIAL logic that is NOT required for your lightning module to run.\n",
    "\n",
    "The following are best practices when using/designing callbacks.\n",
    "\n",
    "* Callbacks should be isolated in their functionality.\n",
    "* Your callback should not rely on the behavior of other callbacks in order to work properly.\n",
    "* Do not manually call methods from the callback.\n",
    "* Directly calling methods (eg. on_validation_end) is strongly discouraged.\n",
    "* Whenever possible, your callbacks should not depend on the order in which they are executed.\n",
    "\n",
    "We will try now to implement a naive version of `MetricTraker` to show how callbacks work. Notice that this is a very easy application of callbacks, fortunately in **PINA** we already provide more advanced callbacks (such as switching optimizer during training).\n",
    "\n",
    "<!-- Suppose we want to log the accuracy on some validation poit -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import Callback\n",
    "import torch\n",
    "\n",
    "# define a simple callback\n",
    "class NaiveMetricTracker(Callback):\n",
    "    def __init__(self):\n",
    "        self.saved_metrics = []\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, __): # function called at the end of each epoch\n",
    "        self.saved_metrics.append(\n",
    "            {key: value for key, value in trainer.logged_metrics.items()}\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the results when applyed to the `SimpleODE` problem. You can define callbacks when initializing the `Trainer` by the `callbacks` argument, which expects a list of callbacks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|██████████| 1/1 [00:00<00:00, 251.26it/s, v_num=1, x0_loss=0.000669, D_loss=0.00561, mean_loss=0.00314]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|██████████| 1/1 [00:00<00:00, 151.22it/s, v_num=1, x0_loss=0.000669, D_loss=0.00561, mean_loss=0.00314]\n"
     ]
    }
   ],
   "source": [
    "model = FeedForward(\n",
    "        layers=[10, 10],\n",
    "        func=torch.nn.Tanh,\n",
    "        output_dimensions=len(problem.output_variables),\n",
    "        input_dimensions=len(problem.input_variables)\n",
    "    )\n",
    "pinn = PINN(problem, model)\n",
    "trainer = Trainer(solver=pinn,\n",
    "                  accelerator='cpu',\n",
    "                  logger=TensorBoardLogger(save_dir='simpleode'),\n",
    "                  enable_model_summary=False,\n",
    "                  callbacks=[NaiveMetricTracker()])  # adding a callbacks\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily access the data by calling `trainer.callbacks[0].saved_metrics` (notice the zero represents the first callback in the list given in the initialization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'x0_loss': tensor(1.6162),\n",
       "  'D_loss': tensor(0.1902),\n",
       "  'mean_loss': tensor(0.9032)},\n",
       " {'x0_loss': tensor(1.5920),\n",
       "  'D_loss': tensor(0.1786),\n",
       "  'mean_loss': tensor(0.8853)},\n",
       " {'x0_loss': tensor(1.5681),\n",
       "  'D_loss': tensor(0.1675),\n",
       "  'mean_loss': tensor(0.8678)}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.callbacks[0].saved_metrics[:3] # only the first three epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pina",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
