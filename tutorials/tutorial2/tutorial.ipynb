{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de19422d",
   "metadata": {},
   "source": [
    "# Tutorial: Two dimensional Poisson problem using Extra Features Learning\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mathLab/PINA/blob/master/tutorials/tutorial2/tutorial.ipynb)\n",
    "\n",
    "This tutorial presents how to solve with Physics-Informed Neural Networks (PINNs) a 2D Poisson problem with Dirichlet boundary conditions. We will train with standard PINN's training, and with extrafeatures. For more insights on extrafeature learning please read [*An extended physics informed neural network for preliminary analysis of parametric optimal control problems*](https://www.sciencedirect.com/science/article/abs/pii/S0898122123002018).\n",
    "\n",
    "First of all, some useful imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0b8dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## routine needed to run the notebook on Google Colab\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "  !pip install \"pina-mathlab\"\n",
    "\n",
    "import torch\n",
    "from torch.nn import Softplus\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('tableau-colorblind10')\n",
    "\n",
    "from pina.problem import SpatialProblem\n",
    "from pina.operator import laplacian\n",
    "from pina.model import FeedForward\n",
    "from pina.solver import PINN\n",
    "from pina.trainer import Trainer\n",
    "from pina.domain import CartesianDomain\n",
    "from pina.equation import Equation, FixedValue\n",
    "from pina import Condition, LabelTensor\n",
    "from pina.callback import MetricTracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492a37b4",
   "metadata": {},
   "source": [
    "## The problem definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0b1777",
   "metadata": {},
   "source": [
    "The two-dimensional Poisson problem is mathematically written as:\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\Delta u = \\sin{(\\pi x)} \\sin{(\\pi y)} \\text{ in } D, \\\\\n",
    "u = 0 \\text{ on } \\Gamma_1 \\cup \\Gamma_2 \\cup \\Gamma_3 \\cup \\Gamma_4,\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "where $D$ is a square domain $[0,1]^2$, and $\\Gamma_i$, with $i=1,...,4$, are the boundaries of the square.\n",
    "\n",
    "The Poisson problem is written in **PINA** code as a class. The equations are written as *conditions* that should be satisfied in the corresponding domains. The *truth_solution*\n",
    "is the exact solution which will be compared with the predicted one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82c24040",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Poisson(SpatialProblem):\n",
    "    output_variables = ['u']\n",
    "    spatial_domain = CartesianDomain({'x': [0, 1], 'y': [0, 1]})\n",
    "\n",
    "    def laplace_equation(input_, output_):\n",
    "        force_term = (torch.sin(input_.extract(['x'])*torch.pi) *\n",
    "                      torch.sin(input_.extract(['y'])*torch.pi))\n",
    "        laplacian_u = laplacian(output_, input_, components=['u'], d=['x', 'y'])\n",
    "        return laplacian_u - force_term\n",
    "\n",
    "    # here we write the problem conditions\n",
    "    conditions = {\n",
    "        'bound_cond1': Condition(domain=CartesianDomain({'x': [0, 1], 'y':  1}), equation=FixedValue(0.)),\n",
    "        'bound_cond2': Condition(domain=CartesianDomain({'x': [0, 1], 'y': 0}), equation=FixedValue(0.)),\n",
    "        'bound_cond3': Condition(domain=CartesianDomain({'x':  1, 'y': [0, 1]}), equation=FixedValue(0.)),\n",
    "        'bound_cond4': Condition(domain=CartesianDomain({'x': 0, 'y': [0, 1]}), equation=FixedValue(0.)),\n",
    "        'phys_cond': Condition(domain=CartesianDomain({'x': [0, 1], 'y': [0, 1]}), equation=Equation(laplace_equation)),\n",
    "    }\n",
    "\n",
    "    def poisson_sol(self, pts):\n",
    "        return -(\n",
    "            torch.sin(pts.extract(['x'])*torch.pi)*\n",
    "            torch.sin(pts.extract(['y'])*torch.pi)\n",
    "        )/(2*torch.pi**2)\n",
    "    \n",
    "    truth_solution = poisson_sol\n",
    "\n",
    "problem = Poisson()\n",
    "\n",
    "# let's discretise the domain\n",
    "problem.discretise_domain(25, 'grid', domains=['phys_cond'])\n",
    "problem.discretise_domain(25, 'grid', domains=['bound_cond1', 'bound_cond2', 'bound_cond3', 'bound_cond4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7086c64d",
   "metadata": {},
   "source": [
    "## Solving the problem with standard PINNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ba4501",
   "metadata": {},
   "source": [
    "After the problem, the feed-forward neural network is defined, through the class `FeedForward`. This neural network takes as input the coordinates (in this case $x$ and $y$) and provides the unkwown field of the Poisson problem. The residual of the equations are evaluated at several sampling points (which the user can manipulate using the method `CartesianDomain_pts`) and the loss minimized by the neural network is the sum of the residuals.\n",
    "\n",
    "In this tutorial, the neural network is composed by two hidden layers of 10 neurons each, and it is trained for 1000 epochs. We use the `MetricTracker` class to track the metrics during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7d20d6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|██████████| 1/1 [00:00<00:00, 21.14it/s, v_num=29, val_loss=0.0929, bound_cond1_loss=0.000311, bound_cond2_loss=0.000309, bound_cond3_loss=0.000195, bound_cond4_loss=0.000268, phys_cond_loss=0.0863, train_loss=0.0874]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|██████████| 1/1 [00:00<00:00, 18.74it/s, v_num=29, val_loss=0.0929, bound_cond1_loss=0.000311, bound_cond2_loss=0.000309, bound_cond3_loss=0.000195, bound_cond4_loss=0.000268, phys_cond_loss=0.0863, train_loss=0.0874]\n"
     ]
    }
   ],
   "source": [
    "# make model + solver + trainer\n",
    "model = FeedForward(\n",
    "    layers=[10, 10],\n",
    "    func=Softplus,\n",
    "    output_dimensions=len(problem.output_variables),\n",
    "    input_dimensions=len(problem.input_variables)\n",
    ")\n",
    "pinn = PINN(problem, model)\n",
    "trainer = Trainer(pinn, max_epochs=1000, callbacks=[MetricTracker()], accelerator='cpu', enable_model_summary=False) # we train on CPU and avoid model summary at beginning of training (optional)\n",
    "\n",
    "# train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb83cc7a",
   "metadata": {},
   "source": [
    "Now the `Plotter` class is used to plot the results.\n",
    "The solution predicted by the neural network is plotted on the left, the exact one is represented at the center and on the right the error between the exact and the predicted solutions is showed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ab83c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotter = Plotter()\n",
    "#plotter.plot(solver=pinn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fdf23e",
   "metadata": {},
   "source": [
    "## Solving the problem with extra-features PINNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e76351",
   "metadata": {},
   "source": [
    "Now, the same problem is solved in a different way.\n",
    "A new neural network is now defined, with an additional input variable, named extra-feature, which coincides with the forcing term in the Laplace equation. \n",
    "The set of input variables to the neural network is:\n",
    "\n",
    "\\begin{equation}\n",
    "[x, y, k(x, y)], \\text{ with } k(x, y)=\\sin{(\\pi x)}\\sin{(\\pi y)},\n",
    "\\end{equation}\n",
    "\n",
    "where $x$ and $y$ are the spatial coordinates and $k(x, y)$ is the added feature. \n",
    "\n",
    "This feature is initialized in the class `SinSin`, which needs to be inherited by the `torch.nn.Module` class and to have the `forward` method. After declaring such feature, we can just adjust the `FeedForward` class by creating a subclass `FeedForwardWithExtraFeatures` with an adjusted forward method and the additional attribute `extra_features`.\n",
    "\n",
    "Finally, we perform the same training as before: the problem is `Poisson`, the network is composed by the same number of neurons and optimizer parameters are equal to previous test, the only change is the new extra feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef3ad372",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|██████████| 1/1 [00:00<00:00, 15.89it/s, v_num=30, val_loss=5.99e-5, bound_cond1_loss=2.74e-6, bound_cond2_loss=3.6e-6, bound_cond3_loss=5.4e-6, bound_cond4_loss=1.07e-5, phys_cond_loss=3.94e-5, train_loss=6.18e-5]     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|██████████| 1/1 [00:00<00:00, 14.05it/s, v_num=30, val_loss=5.99e-5, bound_cond1_loss=2.74e-6, bound_cond2_loss=3.6e-6, bound_cond3_loss=5.4e-6, bound_cond4_loss=1.07e-5, phys_cond_loss=3.94e-5, train_loss=6.18e-5]\n"
     ]
    }
   ],
   "source": [
    "class SinSin(torch.nn.Module):\n",
    "    \"\"\"Feature: sin(x)*sin(y)\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        t = (torch.sin(x.extract(['x'])*torch.pi) *\n",
    "             torch.sin(x.extract(['y'])*torch.pi))\n",
    "        return LabelTensor(t, ['sin(x)sin(y)'])\n",
    "\n",
    "class FeedForwardWithExtraFeatures(FeedForward):\n",
    "    def __init__(self, input_dimensions, output_dimensions, func, layers, extra_features):\n",
    "\n",
    "        super().__init__(input_dimensions=input_dimensions, \n",
    "                         output_dimensions=output_dimensions, \n",
    "                         func=func, \n",
    "                         layers=layers) \n",
    "        self.extra_features = extra_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        extra_feature = self.extra_features[0](x)\n",
    "        x = x.append(extra_feature)\n",
    "        return super().forward(x)\n",
    "    \n",
    "model_feat = FeedForwardWithExtraFeatures(\n",
    "    input_dimensions=len(problem.input_variables) + 1, #we add one as also we consider the extra feature dimension\n",
    "    output_dimensions=len(problem.output_variables),\n",
    "    func=Softplus,\n",
    "    layers=[10, 10],\n",
    "    extra_features=[SinSin()])\n",
    "\n",
    "pinn_feat = PINN(problem, model_feat)\n",
    "trainer_feat = Trainer(pinn_feat, max_epochs=1000, callbacks=[MetricTracker()], accelerator='cpu', enable_model_summary=False) # we train on CPU and avoid model summary at beginning of training (optional)\n",
    "\n",
    "trainer_feat.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9748a13e",
   "metadata": {},
   "source": [
    "The predicted and exact solutions and the error between them are represented below.\n",
    "We can easily note that now our network, having almost the same condition as before, is able to reach additional order of magnitudes in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2be6b145",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotter.plot(solver=pinn_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bc0577",
   "metadata": {},
   "source": [
    "## Solving the problem with learnable extra-features PINNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c1d7b0",
   "metadata": {},
   "source": [
    "We can still do better!\n",
    "\n",
    "Another way to exploit the  extra features is the addition of learnable parameter inside them.\n",
    "In this way, the added parameters are learned during the training phase of the neural network. In this case, we use:\n",
    "\n",
    "\\begin{equation}\n",
    "k(x, \\mathbf{y}) = \\beta \\sin{(\\alpha x)} \\sin{(\\alpha y)},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\alpha$ and $\\beta$ are the abovementioned parameters.\n",
    "Their implementation is quite trivial: by using the class `torch.nn.Parameter` we cam define all the learnable parameters we need, and they are managed by `autograd` module!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae8716e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|██████████| 1/1 [00:00<00:00, 17.90it/s, v_num=31, val_loss=9.7e-5, bound_cond1_loss=1.69e-5, bound_cond2_loss=1.55e-5, bound_cond3_loss=9.96e-6, bound_cond4_loss=6.55e-6, phys_cond_loss=4.93e-5, train_loss=9.82e-5]      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|██████████| 1/1 [00:00<00:00, 15.98it/s, v_num=31, val_loss=9.7e-5, bound_cond1_loss=1.69e-5, bound_cond2_loss=1.55e-5, bound_cond3_loss=9.96e-6, bound_cond4_loss=6.55e-6, phys_cond_loss=4.93e-5, train_loss=9.82e-5]\n"
     ]
    }
   ],
   "source": [
    "class SinSinAB(torch.nn.Module):\n",
    "    \"\"\" \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.alpha = torch.nn.Parameter(torch.tensor([1.0]))\n",
    "        self.beta = torch.nn.Parameter(torch.tensor([1.0]))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        t =  (\n",
    "            self.beta*torch.sin(self.alpha*x.extract(['x'])*torch.pi)*\n",
    "                      torch.sin(self.alpha*x.extract(['y'])*torch.pi)\n",
    "        )\n",
    "        return LabelTensor(t, ['b*sin(a*x)sin(a*y)'])\n",
    "\n",
    "\n",
    "# make model + solver + trainer\n",
    "model_lean = FeedForwardWithExtraFeatures(\n",
    "    input_dimensions=len(problem.input_variables) + 1, #we add one as also we consider the extra feature dimension\n",
    "    output_dimensions=len(problem.output_variables),\n",
    "    func=Softplus,\n",
    "    layers=[10, 10],\n",
    "    extra_features=[SinSinAB()])\n",
    "\n",
    "pinn_lean = PINN(problem, model_lean)\n",
    "trainer_learn = Trainer(pinn_lean, max_epochs=1000, accelerator='cpu', enable_model_summary=False) # we train on CPU and avoid model summary at beginning of training (optional)\n",
    "\n",
    "# train\n",
    "trainer_learn.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0319fb3b",
   "metadata": {},
   "source": [
    "Umh, the final loss is not appreciabily better than previous model (with static extra features), despite the usage of learnable parameters. This is mainly due to the over-parametrization of the network: there are many parameter to optimize during the training, and the model in unable to understand automatically that only the parameters of the extra feature (and not the weights/bias of the FFN) should be tuned in order to fit our problem. A longer training can be helpful, but in this case the faster way to reach machine precision for solving the Poisson problem is removing all the hidden layers in the `FeedForward`, keeping only the $\\alpha$ and $\\beta$ parameters of the extra feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daa9cf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|██████████| 1/1 [00:00<00:00, 14.49it/s, v_num=32, val_loss=0.121, bound_cond1_loss=0.000655, bound_cond2_loss=0.00118, bound_cond3_loss=0.000742, bound_cond4_loss=0.000474, phys_cond_loss=0.108, train_loss=0.111]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|██████████| 1/1 [00:00<00:00, 12.95it/s, v_num=32, val_loss=0.121, bound_cond1_loss=0.000655, bound_cond2_loss=0.00118, bound_cond3_loss=0.000742, bound_cond4_loss=0.000474, phys_cond_loss=0.108, train_loss=0.111]\n"
     ]
    }
   ],
   "source": [
    "# make model + solver + trainer\n",
    "model_lean= FeedForwardWithExtraFeatures(\n",
    "    layers=[],\n",
    "    func=Softplus,\n",
    "    output_dimensions=len(problem.output_variables),\n",
    "    input_dimensions=len(problem.input_variables)+1,\n",
    "    extra_features=[SinSinAB()])\n",
    "pinn_learn = PINN(problem, model_lean)\n",
    "trainer_learn = Trainer(pinn_learn, max_epochs=1000, callbacks=[MetricTracker()], accelerator='cpu', enable_model_summary=False) # we train on CPU and avoid model summary at beginning of training (optional)\n",
    "\n",
    "# train\n",
    "trainer_learn.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150b3e62",
   "metadata": {},
   "source": [
    "In such a way, the model is able to reach a very high accuracy!\n",
    "Of course, this is a toy problem for understanding the usage of extra features: similar precision could be obtained if the extra features are very similar to the true solution. The analyzed Poisson problem shows a forcing term very close to the solution, resulting in a perfect problem to address with such an approach.\n",
    "\n",
    "We conclude here by showing the graphical comparison of the unknown field and the loss trend for all the test cases presented here: the standard PINN, PINN with extra features, and PINN with learnable extra features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96e51c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotter.plot(solver=pinn_learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c64fcb4",
   "metadata": {},
   "source": [
    "Let us compare the training losses for the various types of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2855cea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_metrics = trainer.callbacks[list_[0]].metrics\n",
    "\n",
    "loss = trainer_metrics['val_loss']\n",
    "epochs = range(len(loss))\n",
    "plt.plot(epochs, loss.cpu())\n",
    "# plotting\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.yscale('log')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4c8895",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "Congratulations on completing the two dimensional Poisson tutorial of **PINA**! There are multiple directions you can go now:\n",
    "\n",
    "1. Train the network for longer or with different layer sizes and assert the finaly accuracy\n",
    "\n",
    "2. Propose new types of extrafeatures and see how they affect the learning\n",
    "\n",
    "3. Exploit extrafeature training in more complex problems\n",
    "\n",
    "4. Many more..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
